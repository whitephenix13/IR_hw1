{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 Code and Report\n",
    "AUFAR Rezka and LASBLEIS Alexandre "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the first step you will generate pair s of rankings of relevance, for the production P and experimental E,\n",
    "respectively, for a hypothetical query q . Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all\n",
    "possible P and E ranking pairs of length 5, for which E outperforms P.\n",
    "\n",
    "Example:<br>\n",
    "P: {N N N N N}<br>\n",
    "E: {N N N N R}<br>\n",
    "… <br>\n",
    "P: {HR HR HR HR R}<br>\n",
    "E: {HR HR HR HR HR}<br>\n",
    "\n",
    "(Note 1: If you do not have enough computational power, sample 1000 pair uniformly at random to show\n",
    "your work.)\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert int to relevance\n",
    "#_list : list of integer to be converted\n",
    "#return : list of string where every integer has been replaced by its relevance interpretation\n",
    "def to_string(_list): \n",
    "    L=[]\n",
    "    for i in _list:\n",
    "        if i==0:\n",
    "            L.append(\"N \")\n",
    "        elif i==1:\n",
    "            L.append(\"R \")\n",
    "        elif i==2:\n",
    "            L.append(\"HR\")\n",
    "        else: \n",
    "            L.append(\"? \")\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs = [] #variable used to store all pairs \n",
    "Ps = []\n",
    "Es = []\n",
    "\n",
    "#Create all combinaison for P, there are 3^5 = 243 possibilities since P is of length 5 and each member have 3 possible values:\n",
    "# 0 (representing \"N\" ), 1 (representing \"R), 2 (representing \"HR)\n",
    "for p_possibility in range(243): \n",
    "    P=[None]*5\n",
    "    #create all five values for P \n",
    "    for p_pos in range(5):\n",
    "        P[p_pos]= p_possibility // (np.power(3,p_pos)) %3 #formula to create all combinaison\n",
    "    #we do the same for E    \n",
    "    for e_possibility in range(243): \n",
    "        E=[None]*5\n",
    "        for e_pos in range(5):\n",
    "            E[e_pos]= e_possibility // (np.power(3,e_pos)) %3 #formula to create all combinaison\n",
    "        pairs.append((P,E)) #adding the pair since every pair is valid\n",
    "        #Memorize all created Es but only once\n",
    "        if p_possibility==0:\n",
    "            Es.append(E)\n",
    "        #memorize all created Ps\n",
    "    Ps.append(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59049\n",
      "('P: ', ['N ', 'HR', 'N ', 'N ', 'HR'], 'E: ', ['N ', 'R ', 'N ', 'R ', 'N '])\n",
      "243\n",
      "[[0, 0, 0, 0, 0], [1, 0, 0, 0, 0], [2, 0, 0, 0, 0], [0, 1, 0, 0, 0], [1, 1, 0, 0, 0], [2, 1, 0, 0, 0], [0, 2, 0, 0, 0], [1, 2, 0, 0, 0], [2, 2, 0, 0, 0], [0, 0, 1, 0, 0], [1, 0, 1, 0, 0], [2, 0, 1, 0, 0], [0, 1, 1, 0, 0], [1, 1, 1, 0, 0], [2, 1, 1, 0, 0], [0, 2, 1, 0, 0], [1, 2, 1, 0, 0], [2, 2, 1, 0, 0], [0, 0, 2, 0, 0], [1, 0, 2, 0, 0], [2, 0, 2, 0, 0], [0, 1, 2, 0, 0], [1, 1, 2, 0, 0], [2, 1, 2, 0, 0], [0, 2, 2, 0, 0], [1, 2, 2, 0, 0], [2, 2, 2, 0, 0], [0, 0, 0, 1, 0], [1, 0, 0, 1, 0], [2, 0, 0, 1, 0], [0, 1, 0, 1, 0], [1, 1, 0, 1, 0], [2, 1, 0, 1, 0], [0, 2, 0, 1, 0], [1, 2, 0, 1, 0], [2, 2, 0, 1, 0], [0, 0, 1, 1, 0], [1, 0, 1, 1, 0], [2, 0, 1, 1, 0], [0, 1, 1, 1, 0], [1, 1, 1, 1, 0], [2, 1, 1, 1, 0], [0, 2, 1, 1, 0], [1, 2, 1, 1, 0], [2, 2, 1, 1, 0], [0, 0, 2, 1, 0], [1, 0, 2, 1, 0], [2, 0, 2, 1, 0], [0, 1, 2, 1, 0], [1, 1, 2, 1, 0], [2, 1, 2, 1, 0], [0, 2, 2, 1, 0], [1, 2, 2, 1, 0], [2, 2, 2, 1, 0], [0, 0, 0, 2, 0], [1, 0, 0, 2, 0], [2, 0, 0, 2, 0], [0, 1, 0, 2, 0], [1, 1, 0, 2, 0], [2, 1, 0, 2, 0], [0, 2, 0, 2, 0], [1, 2, 0, 2, 0], [2, 2, 0, 2, 0], [0, 0, 1, 2, 0], [1, 0, 1, 2, 0], [2, 0, 1, 2, 0], [0, 1, 1, 2, 0], [1, 1, 1, 2, 0], [2, 1, 1, 2, 0], [0, 2, 1, 2, 0], [1, 2, 1, 2, 0], [2, 2, 1, 2, 0], [0, 0, 2, 2, 0], [1, 0, 2, 2, 0], [2, 0, 2, 2, 0], [0, 1, 2, 2, 0], [1, 1, 2, 2, 0], [2, 1, 2, 2, 0], [0, 2, 2, 2, 0], [1, 2, 2, 2, 0], [2, 2, 2, 2, 0], [0, 0, 0, 0, 1], [1, 0, 0, 0, 1], [2, 0, 0, 0, 1], [0, 1, 0, 0, 1], [1, 1, 0, 0, 1], [2, 1, 0, 0, 1], [0, 2, 0, 0, 1], [1, 2, 0, 0, 1], [2, 2, 0, 0, 1], [0, 0, 1, 0, 1], [1, 0, 1, 0, 1], [2, 0, 1, 0, 1], [0, 1, 1, 0, 1], [1, 1, 1, 0, 1], [2, 1, 1, 0, 1], [0, 2, 1, 0, 1], [1, 2, 1, 0, 1], [2, 2, 1, 0, 1], [0, 0, 2, 0, 1], [1, 0, 2, 0, 1], [2, 0, 2, 0, 1], [0, 1, 2, 0, 1], [1, 1, 2, 0, 1], [2, 1, 2, 0, 1], [0, 2, 2, 0, 1], [1, 2, 2, 0, 1], [2, 2, 2, 0, 1], [0, 0, 0, 1, 1], [1, 0, 0, 1, 1], [2, 0, 0, 1, 1], [0, 1, 0, 1, 1], [1, 1, 0, 1, 1], [2, 1, 0, 1, 1], [0, 2, 0, 1, 1], [1, 2, 0, 1, 1], [2, 2, 0, 1, 1], [0, 0, 1, 1, 1], [1, 0, 1, 1, 1], [2, 0, 1, 1, 1], [0, 1, 1, 1, 1], [1, 1, 1, 1, 1], [2, 1, 1, 1, 1], [0, 2, 1, 1, 1], [1, 2, 1, 1, 1], [2, 2, 1, 1, 1], [0, 0, 2, 1, 1], [1, 0, 2, 1, 1], [2, 0, 2, 1, 1], [0, 1, 2, 1, 1], [1, 1, 2, 1, 1], [2, 1, 2, 1, 1], [0, 2, 2, 1, 1], [1, 2, 2, 1, 1], [2, 2, 2, 1, 1], [0, 0, 0, 2, 1], [1, 0, 0, 2, 1], [2, 0, 0, 2, 1], [0, 1, 0, 2, 1], [1, 1, 0, 2, 1], [2, 1, 0, 2, 1], [0, 2, 0, 2, 1], [1, 2, 0, 2, 1], [2, 2, 0, 2, 1], [0, 0, 1, 2, 1], [1, 0, 1, 2, 1], [2, 0, 1, 2, 1], [0, 1, 1, 2, 1], [1, 1, 1, 2, 1], [2, 1, 1, 2, 1], [0, 2, 1, 2, 1], [1, 2, 1, 2, 1], [2, 2, 1, 2, 1], [0, 0, 2, 2, 1], [1, 0, 2, 2, 1], [2, 0, 2, 2, 1], [0, 1, 2, 2, 1], [1, 1, 2, 2, 1], [2, 1, 2, 2, 1], [0, 2, 2, 2, 1], [1, 2, 2, 2, 1], [2, 2, 2, 2, 1], [0, 0, 0, 0, 2], [1, 0, 0, 0, 2], [2, 0, 0, 0, 2], [0, 1, 0, 0, 2], [1, 1, 0, 0, 2], [2, 1, 0, 0, 2], [0, 2, 0, 0, 2], [1, 2, 0, 0, 2], [2, 2, 0, 0, 2], [0, 0, 1, 0, 2], [1, 0, 1, 0, 2], [2, 0, 1, 0, 2], [0, 1, 1, 0, 2], [1, 1, 1, 0, 2], [2, 1, 1, 0, 2], [0, 2, 1, 0, 2], [1, 2, 1, 0, 2], [2, 2, 1, 0, 2], [0, 0, 2, 0, 2], [1, 0, 2, 0, 2], [2, 0, 2, 0, 2], [0, 1, 2, 0, 2], [1, 1, 2, 0, 2], [2, 1, 2, 0, 2], [0, 2, 2, 0, 2], [1, 2, 2, 0, 2], [2, 2, 2, 0, 2], [0, 0, 0, 1, 2], [1, 0, 0, 1, 2], [2, 0, 0, 1, 2], [0, 1, 0, 1, 2], [1, 1, 0, 1, 2], [2, 1, 0, 1, 2], [0, 2, 0, 1, 2], [1, 2, 0, 1, 2], [2, 2, 0, 1, 2], [0, 0, 1, 1, 2], [1, 0, 1, 1, 2], [2, 0, 1, 1, 2], [0, 1, 1, 1, 2], [1, 1, 1, 1, 2], [2, 1, 1, 1, 2], [0, 2, 1, 1, 2], [1, 2, 1, 1, 2], [2, 2, 1, 1, 2], [0, 0, 2, 1, 2], [1, 0, 2, 1, 2], [2, 0, 2, 1, 2], [0, 1, 2, 1, 2], [1, 1, 2, 1, 2], [2, 1, 2, 1, 2], [0, 2, 2, 1, 2], [1, 2, 2, 1, 2], [2, 2, 2, 1, 2], [0, 0, 0, 2, 2], [1, 0, 0, 2, 2], [2, 0, 0, 2, 2], [0, 1, 0, 2, 2], [1, 1, 0, 2, 2], [2, 1, 0, 2, 2], [0, 2, 0, 2, 2], [1, 2, 0, 2, 2], [2, 2, 0, 2, 2], [0, 0, 1, 2, 2], [1, 0, 1, 2, 2], [2, 0, 1, 2, 2], [0, 1, 1, 2, 2], [1, 1, 1, 2, 2], [2, 1, 1, 2, 2], [0, 2, 1, 2, 2], [1, 2, 1, 2, 2], [2, 2, 1, 2, 2], [0, 0, 2, 2, 2], [1, 0, 2, 2, 2], [2, 0, 2, 2, 2], [0, 1, 2, 2, 2], [1, 1, 2, 2, 2], [2, 1, 2, 2, 2], [0, 2, 2, 2, 2], [1, 2, 2, 2, 2], [2, 2, 2, 2, 2]]\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "print(len(pairs)) #= 59049 which is equal to the expected length = 3 ^10 = 59049\n",
    "#print(pairs) #print all pairs but this takes a lot of place\n",
    "\n",
    "#print a random pair to show that it makes sense \n",
    "r = rand.randint(0,59049-1)\n",
    "print(\"P: \" , to_string(pairs[r][0]), \"E: \",to_string(pairs[r][1]))\n",
    "\n",
    "#check if Es is as expected \n",
    "print(len(Es))#expect 243 = 3^5 \n",
    "print(Es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanations\n",
    "In order to contruct all possible pairs of P and E with a 3-graded relevance {N, R, HR}, we first encode those relevance as integers: <br>\n",
    "* 0 for N <br>\n",
    "* 1 for R <br>\n",
    "* 2 for HR <br>\n",
    "This encoding allows us to do easy comparison between elements.\n",
    "Then we created all pairs using this formula : $value = \\frac{possibility}{3^{pos}} [3]$ with $possibility \\in \\{0,...,243\\}$ and $ pos \\in \\{0,1,2,3,4\\}$.\n",
    "\n",
    "This give the following sequence depending on the position of the element in the list: <br>\n",
    "* 0: $\\{0,1,2,0,1,2,0,1,2,0,1,2...\\}$\n",
    "* 1: $\\{0,0,0,1,1,1,2,2,2,0,0,0...\\}$\n",
    "* 2: $\\{0,0,0,0,0,0,0,0,0,1,1,1,...\\}$\n",
    "* ... and so on\n",
    "\n",
    "This formula is then used for the production P and the experiment E. \n",
    "\n",
    "##### Analysis\n",
    "According to the formula we showed before, we should find all the possible combination. Futhermore, the number of pairs found is equal to the one we expected $3^{10}$ and the number of E and P are $3^{5}$ each which is also what is expected. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 2 : Implement Evaluation Measures (15 points)\n",
    "\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above.\n",
    "\n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant\n",
    "documents in the entire collection – pay extra attention on how to find this)\n",
    "\n",
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Remove for evaluation \n",
    "\n",
    "#Recall at rank k \n",
    "def Binary_measure_recal(test_set, r):\n",
    "    num_test_rel = 0 \n",
    "    total_num_rel=0\n",
    "    for i in range(len(test_set)):\n",
    "        if test_set[i]>0:\n",
    "            if i<r:\n",
    "                num_test_rel+=1\n",
    "            total_num_rel+=1\n",
    "    return float(num_test_rel) / total_num_rel\n",
    "\n",
    "#Average precision at rank k \n",
    "def Binary_measure_avg_precision(test_set, r):\n",
    "    num_rel=0\n",
    "    sum_precision = 0 \n",
    "    total_num_rel=0\n",
    "    temp=\"(\"\n",
    "    for i in range(len(test_set)):\n",
    "        if test_set[i]>0:\n",
    "            if i<r:\n",
    "                num_rel+=1\n",
    "                temp+=\" \"+str(num_rel)+\"/\"+str(i+1)+\"+\"\n",
    "                sum_precision+=float(num_rel)/(i+1)\n",
    "            total_num_rel+=1\n",
    "    temp=temp[:len(temp)-1]+\")\"\n",
    "    #print(temp,\"/\",total_num_rel)\n",
    "    return float(sum_precision) / total_num_rel\n",
    "#Normalized discount cumulative gain nDCG at rank r\n",
    "def nDCG(test_set, r):\n",
    "    orderedSet= list(test_set)\n",
    "    orderedSet.sort(reverse=True)\n",
    "    return DCG(test_set, r) / DCG(orderedSet, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BINARY \n",
    "#Precision at rank r \n",
    "#test_set: the collection to be tested \n",
    "#r: the rank for which we test the collection \n",
    "#return : the precision \n",
    "def Binary_measure_precision(test_set, r):\n",
    "    assert r <= len(test_set)\n",
    "    assert r!= 0\n",
    "    num_test_rel = 0 \n",
    "    for i in range(r):\n",
    "        #According to our notation, if test_set[i]>0, the document is relevant\n",
    "        if test_set[i]>0:\n",
    "            num_test_rel+=1\n",
    "    return float(num_test_rel) / r #precision is number of relevant doc/ number of doc considered\n",
    "\n",
    "#MULTI GRADED \n",
    "#Discount cumulative gain DCG at rank r\n",
    "#test_set: the collection to be tested \n",
    "#r: the rank for which we test the collection \n",
    "#return : the gain \n",
    "def DCG(test_set, r):\n",
    "    assert r <= len(test_set)\n",
    "    assert r!= 0\n",
    "    res=0\n",
    "    for k in range(1,r+1):#r is the rank so it starts at 1\n",
    "        rel_r = test_set[k-1]#index of element = rank -1 since rank starts at 1 and index at 0\n",
    "        res+=float(np.power(2,rel_r)-1)/(np.log2(1+k))\n",
    "    return res\n",
    "\n",
    "#Rank Biased Precision w ith persistence parameter 𝜃 =0.8\n",
    "#test_set: the collection to be tested \n",
    "#theta: the theta parameter of the RBP formula\n",
    "#return : the precision\n",
    "def RBP (test_set,theta=0.8):\n",
    "    res=0\n",
    "    for k in range(1,len(test_set)+1):\n",
    "        rel_k=test_set[k-1]\n",
    "        res+= rel_k * np.power(theta,k-1) * (1.0-theta)\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific ordering\n",
      "('Rank = ', 1, 'Precision', 1.0, 'DCG', 1.0, 'RBP', 1.06)\n",
      "('Rank = ', 2, 'Precision', 0.5, 'DCG', 1.0, 'RBP', 1.06)\n",
      "('Rank = ', 3, 'Precision', 0.667, 'DCG', 2.5, 'RBP', 1.06)\n",
      "('Rank = ', 4, 'Precision', 0.75, 'DCG', 15.851, 'RBP', 1.06)\n",
      "('Rank = ', 5, 'Precision', 0.6, 'DCG', 15.851, 'RBP', 1.06)\n",
      "('Rank = ', 6, 'Precision', 0.667, 'DCG', 16.207, 'RBP', 1.06)\n",
      "('Rank = ', 7, 'Precision', 0.571, 'DCG', 16.207, 'RBP', 1.06)\n",
      "('Rank = ', 8, 'Precision', 0.5, 'DCG', 16.207, 'RBP', 1.06)\n",
      "('Rank = ', 9, 'Precision', 0.444, 'DCG', 16.207, 'RBP', 1.06)\n",
      "('Rank = ', 10, 'Precision', 0.5, 'DCG', 16.496, 'RBP', 1.06)\n",
      "\n",
      "Perfect ordering\n",
      "('Rank = ', 1, 'Precision', 1.0, 'DCG', 31.0, 'RBP', 1.632)\n",
      "('Rank = ', 2, 'Precision', 1.0, 'DCG', 32.893, 'RBP', 1.632)\n",
      "('Rank = ', 3, 'Precision', 1.0, 'DCG', 33.393, 'RBP', 1.632)\n",
      "('Rank = ', 4, 'Precision', 1.0, 'DCG', 33.823, 'RBP', 1.632)\n",
      "('Rank = ', 5, 'Precision', 1.0, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 6, 'Precision', 0.833, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 7, 'Precision', 0.714, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 8, 'Precision', 0.625, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 9, 'Precision', 0.556, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 10, 'Precision', 0.5, 'DCG', 34.21, 'RBP', 1.632)\n",
      "\n",
      "Worst ordering\n",
      "('Rank = ', 1, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 2, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 3, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 4, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 5, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 6, 'Precision', 0.167, 'DCG', 0.356, 'RBP', 0.361)\n",
      "('Rank = ', 7, 'Precision', 0.286, 'DCG', 0.69, 'RBP', 0.361)\n",
      "('Rank = ', 8, 'Precision', 0.375, 'DCG', 1.005, 'RBP', 0.361)\n",
      "('Rank = ', 9, 'Precision', 0.444, 'DCG', 1.908, 'RBP', 0.361)\n",
      "('Rank = ', 10, 'Precision', 0.5, 'DCG', 10.869, 'RBP', 0.361)\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "test=        [1,0,2,5,0,1,0,0,0,1]#length 10\n",
    "test_perfect=[5,2,1,1,1,0,0,0,0,0]\n",
    "test_worst=  [0,0,0,0,0,1,1,1,2,5]\n",
    "\n",
    "#The following results are rounded in order to make them readable\n",
    "#Specific ordering \n",
    "print(\"Specific ordering\")\n",
    "for r in range(1,len(test)+1):\n",
    "    print('Rank = ' , r,'Precision',round(Binary_measure_precision(test,r),3),'DCG', round(DCG(test,r),3),\n",
    "          'RBP', round(RBP(test),3))\n",
    "\n",
    "#Perfect ordering \n",
    "print(\"\\nPerfect ordering\")\n",
    "for r in range(1,len(test_perfect)+1):\n",
    "    print('Rank = ' , r,'Precision',round(Binary_measure_precision(test_perfect,r),3),'DCG', round(DCG(test_perfect,r),3),\n",
    "          'RBP', round(RBP(test_perfect),3))\n",
    "\n",
    "#Worst ordering \n",
    "print(\"\\nWorst ordering\")\n",
    "for r in range(1,len(test_worst)+1):\n",
    "    print('Rank = ' , r,'Precision',round(Binary_measure_precision(test_worst,r),3),'DCG', round(DCG(test_worst,r),3),\n",
    "          'RBP', round(RBP(test_worst),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanations\n",
    "For Precision and DCG, we need to ensure that rank k does not exceed the documents length, and k cannot be zero.<br>\n",
    "For DCG and RBP, we also decided that the value of the relevant would be equal to the value at that rank in the list. For example if an example is extremely relevant, he might get a value of 5 (as in the example) which leads to higher gain in DCG than a document which is only highly relevant (value of 2).<br>\n",
    "Otherwise our implementation follows the definition of the method. \n",
    "\n",
    "##### Analysis\n",
    "The result for Precision is highly dependant on the number of non-relevant in the cut-off collection. As k increases, the more non-relevant exists in the cut-off, the lower the precision will be. If we consider a rank equal to the length of the list, the precision is the same whatever the order is. Precision does not that into account how good the relevance of a document is, it only cares if a document is relevant or not. \n",
    "\n",
    "For DCG, the result depends on the degree of the relevance and the order of it. If the highest relevant score exists in the first position of the cut-off, the higher DCG will be. We showed that by displaying the DCG score with perfect ordering which is higher at any rank than the \"specific ordering\". Those value may be hard to use to compare different collections due to the fact that it is sensitive to the degree of relevance. A way to cope with that problem is to use its alternative version: the nDCG (normalized version). \n",
    "\n",
    "For RBP, we disregard rank and therefore we have the same result no matter the rank is. This measure pays great attention to the ordering as shown above: the best score is obtained with a perfect ordering whereas the worst one is obtained with the worst ordering. The model behind RBP considers that when you see a document, you have a probability to see the next document otherwise you stop. This model can be improved by taking into account how relevant is the document that we are looking at. Depending on this relevance, there is a probability to see the next document or to stop. This second model is called ERR model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Calculate the measure (5 points)\n",
    "\n",
    "For the three measures and all P and E ranking pairs constructed above calculate the difference: 𝛥measure\n",
    "= measure E -measure P . Consider only those pairs for which E outperforms P.\n",
    "\n",
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluate if E outperfoms P with respect to a specific measure. \n",
    "#methond_name: the measure used to test. It can be \"precision\", \"DCG\" or \"RBP\"\n",
    "#E: \n",
    "#P:\n",
    "#rank: \n",
    "#return : (outperforms, delta measure) where outperforms is True if E outperfors P with respect to the measure method for the\n",
    "#rank and delta measure is the difference between to measure of E and P for the measure method.\n",
    "def outperforms(methond_name,E,P,rank):\n",
    "    if methond_name==\"precision\":\n",
    "        delta = Binary_measure_precision(E,rank)-Binary_measure_precision(P,rank)\n",
    "        return (delta>0), delta \n",
    "    elif methond_name==\"DCG\":\n",
    "        delta=DCG(E,rank)-DCG(P,rank)\n",
    "        return (delta>0), delta \n",
    "    elif methond_name==\"RBP\":\n",
    "        delta=RBP(E)-RBP(P)\n",
    "        return (delta>0), delta \n",
    "\n",
    "#variable for precision measure \n",
    "outperf_pairs_prec=[] #memorize pairs for which E outperfoms P\n",
    "pairs_set_prec = [] \n",
    "d_measure_prec=[] #memorize \"measure(E)-measure(P)\" for which E outperfoms P\n",
    "\n",
    "#variable for DCG measure \n",
    "outperf_pairs_DCG=[] #memorize pairs for which E outperfoms P\n",
    "pairs_set_DCG = []\n",
    "d_measure_DCG=[] #memorize \"measure(E)-measure(P)\" for which E outperfoms P\n",
    "\n",
    "#variable for RBP measure\n",
    "outperf_pairs_RBP=[] #memorize pairs for which E outperfoms P\n",
    "pairs_set_RBP = []\n",
    "d_measure_RBP=[] #memorize \"measure(E)-measure(P)\" for which E outperfoms P\n",
    "\n",
    "rank = 4\n",
    "for i in range(len(pairs)):#pairs made of (P,E)\n",
    "    #test if E outperforms P with respect to a specific method\n",
    "    outperforms_prec,delta_prec= outperforms(\"precision\",pairs[i][1],pairs[i][0],rank)\n",
    "    outperforms_DCG,delta_DCG= outperforms(\"DCG\",pairs[i][1],pairs[i][0],rank)\n",
    "    outperforms_RBP,delta_RBP= outperforms(\"RBP\",pairs[i][1],pairs[i][0],rank)\n",
    "\n",
    "    #Memorize some \"precision\" information only if E outperfom P\n",
    "    if(outperforms_prec):\n",
    "        outperf_pairs_prec.append(pairs[i])\n",
    "        temp = pairs[i][1] + pairs[i][0]\n",
    "        pairs_set_prec.append(''.join(str(x) for x in temp))\n",
    "        d_measure_prec.append(delta_prec)\n",
    "    \n",
    "    #Memorize some \"DCG\" information only if E outperfom P\n",
    "    if(outperforms_DCG):\n",
    "        outperf_pairs_DCG.append(pairs[i])\n",
    "        temp = pairs[i][1] + pairs[i][0]\n",
    "        pairs_set_DCG.append(''.join(str(x) for x in temp))\n",
    "        d_measure_DCG.append(delta_DCG)\n",
    "    \n",
    "    #Memorize some \"RBP\" information only if E outperfom P\n",
    "    if(outperforms_RBP):\n",
    "        outperf_pairs_RBP.append(pairs[i])\n",
    "        temp = pairs[i][1] + pairs[i][0]\n",
    "        pairs_set_RBP.append(''.join(str(x) for x in temp))\n",
    "        d_measure_RBP.append(delta_RBP)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs for which E outperforms P for all three measures:  17184\n",
      "Pair in precision and in DCG: ['1102100122']\n",
      "Pair in precision and not in DCG: ['1111222102']\n",
      "Pair in DCG and not in precision: ['2222212111']\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "\n",
    "print (\"Number of pairs for which E outperforms P for all three measures: \",len(set.intersection(set(pairs_set_prec), set(pairs_set_DCG), set(pairs_set_RBP))))\n",
    "print (\"Pair in precision and in DCG:\",rand.sample(set.intersection(set(pairs_set_prec), set(pairs_set_DCG)),1))\n",
    "\n",
    "# pick result that is in prec but is not in DCG\n",
    "print (\"Pair in precision and not in DCG:\",rand.sample(set(pairs_set_prec) - set(pairs_set_DCG),1))\n",
    "# pick result that is in DCG but is not in prec\n",
    "print (\"Pair in DCG and not in precision:\",rand.sample(set(pairs_set_DCG) - set(pairs_set_prec),1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pair in precision and not in DCG:', [[1, 1, 1, 1, 2], [2, 2, 1, 0, 2]], 'precision E:', 1.0, 'precision P:', 0.75, 'DCG E: ', 2.5616063116448506, 'DCG P: ', 5.3927892607143724)\n",
      "('Pair in DCG and not in precision:', [[2, 2, 2, 2, 2], [1, 2, 1, 1, 1]], 'DCG E: ', 7.6848189349345519, 'DCG P: ', 3.8234658187877653, 'precision E:', 1.0, 'precision P:', 1.0)\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "pair_prec_not_DCG = [[1,1,1,1,2],[2,2,1,0,2]] # pair E, P \n",
    "pair_DCG_not_prec = [[2,2,2,2,2],[1,2,1,1,1]] # pair E, P \n",
    "print(\"Pair in precision and not in DCG:\",pair_prec_not_DCG, \"precision E:\",\n",
    "      Binary_measure_precision(pair_prec_not_DCG[0],rank),\"precision P:\",\n",
    "      Binary_measure_precision(pair_prec_not_DCG[1],rank),\"DCG E: \",\n",
    "     DCG(pair_prec_not_DCG[0],rank),\"DCG P: \",\n",
    "     DCG(pair_prec_not_DCG[1],rank) )\n",
    "\n",
    "print(\"Pair in DCG and not in precision:\",pair_DCG_not_prec, \"DCG E: \",\n",
    "     DCG(pair_DCG_not_prec[0],rank),\"DCG P: \",\n",
    "     DCG(pair_DCG_not_prec[1],rank),\"precision E:\",\n",
    "      Binary_measure_precision(pair_DCG_not_prec[0],rank),\"precision P:\",\n",
    "      Binary_measure_precision(pair_DCG_not_prec[1],rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanations\n",
    "For precision, DCG, and RBP, we calculate the difference between all combination of E and P that we have generated in Step 1, then we append every difference that is greater than zero, which means that E outperforms P.\n",
    "##### Analysis\n",
    "For each three measures, we have different pairs in which E outperforms P. There are 17184 pairs where E outperforms P in all three measures. However, there exists some pairs where E outperforms P for one kind of measure, but not the other. <br> \n",
    "For example, E=[1,1,1,1,2] outperforms P=[2,2,1,0,2] in precision (delta of 0.25), but not in DCG (delta of -2.83118294971) . This is due to the fact that precision does not check order and the degree of relevance. The same happens the opposite way : E=[2,2,2,2,2] outperforms P=[1,2,1,1,1] in DCG (delta of 3.86135311614), but not in precision (delta of 0). <br>\n",
    "This indicates that in real world application, it is important to implement different kinds of measure so we will have broader feedback to optimize the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 4 : Implement Interleave (15 points)\n",
    "\n",
    "Implement 2 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, AND (2),\n",
    "Probabilistic Interleaving. The interleaving algorithms should (a) given two rankings of relevance\n",
    "interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign\n",
    "credit to the algorithms that produced the rankings.\n",
    "\n",
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assignment assumption : E and P always return different documents\n",
    "# general parameter : consider two pairs can contain same documents \n",
    "\n",
    "def team_draft_interleave(ranking_pairs, general=False):\n",
    "    # interleave two list\n",
    "    interleaved_list = []\n",
    "    relevance_list = []\n",
    "    added_list = []\n",
    "    assert len(ranking_pairs[0]) ==  len(ranking_pairs[1])\n",
    "    for i, elem in enumerate(ranking_pairs[0]):\n",
    "        coin = rand.choice([0,1])\n",
    "        if(coin == 0):\n",
    "            if(general and ranking_pairs[0][i] not in added_list):\n",
    "                interleaved_list.append((ranking_pairs[0][i], \"E\"))\n",
    "                added_list.append(ranking_pairs[0][i])\n",
    "            if(general and ranking_pairs[1][i] not in added_list):\n",
    "                interleaved_list.append((ranking_pairs[1][i], \"P\"))\n",
    "                added_list.append(ranking_pairs[1][i])\n",
    "            if(not general):\n",
    "                interleaved_list.append((ranking_pairs[0][i], \"E\"))\n",
    "                interleaved_list.append((ranking_pairs[1][i], \"P\"))\n",
    "                relevance_list.append(ranking_pairs[0][i])\n",
    "                relevance_list.append(ranking_pairs[1][i])\n",
    "        if(coin == 1):\n",
    "            if(general and ranking_pairs[1][i] not in added_list):\n",
    "                interleaved_list.append((ranking_pairs[1][i], \"P\"))\n",
    "                added_list.append(ranking_pairs[1][i])\n",
    "            if(general and ranking_pairs[0][i] not in added_list):\n",
    "                interleaved_list.append((ranking_pairs[0][i], \"E\"))\n",
    "                added_list.append(ranking_pairs[0][i])\n",
    "            if(not general):\n",
    "                interleaved_list.append((ranking_pairs[1][i], \"P\"))\n",
    "                interleaved_list.append((ranking_pairs[0][i], \"E\"))\n",
    "                relevance_list.append(ranking_pairs[1][i])\n",
    "                relevance_list.append(ranking_pairs[0][i])\n",
    "    return interleaved_list, relevance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(6, 'P'), (1, 'E'), (7, 'P'), (2, 'E'), (3, 'E'), (8, 'P'), (4, 'E'), (5, 'E')], [])\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "# tested with general case, which E and P can contain same link\n",
    "general_test = ([1,2,3,4,5], [6,7,8,1,2])\n",
    "print team_draft_interleave(general_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 1, 3, 2, 6]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "# tested the result with existing library (https://github.com/mpkato/interleaving/tree/master/interleaving)\n",
    "\n",
    "import interleaving\n",
    "a = [1, 2, 3, 4, 5] # Ranking 1\n",
    "b = [6, 7, 8, 9, 10] # Ranking 2\n",
    "method = interleaving.Probabilistic([a, b]) # initialize an interleaving method\n",
    "ranking = method.interleave() # interleaving\n",
    "print ranking\n",
    "\n",
    "clicks = [0, 3] # observed clicks, i.e. documents 1 and 2 are clicked\n",
    "result = interleaving.Probabilistic.evaluate(ranking, clicks)\n",
    "print result # (0, 1) indicates Ranking 1 won Ranking 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def softmax(tau, length):\n",
    "    total_denominator = 0\n",
    "    softmax_distribution = []\n",
    "    for i in range(1, length+1):\n",
    "        total_denominator += float(1.0/np.power(i,tau))\n",
    "    for i in range(1, length+1):\n",
    "        softmax_distribution.append(float(1.0/np.power(i,tau) / (total_denominator)))\n",
    "    return softmax_distribution\n",
    "\n",
    "def probabilistic_interleave(ranking_pairs):\n",
    "    # generate softmax probability distribution with tau = 3 (Hofmann, 2011)\n",
    "    assert len(ranking_pairs[0]) ==  len(ranking_pairs[1])\n",
    "    E = copy.copy(ranking_pairs[0])\n",
    "    P = copy.copy(ranking_pairs[1])\n",
    "    softmax_distribution_E = softmax(3,len(E))\n",
    "    softmax_distribution_P = softmax(3,len(P))\n",
    "    # interleave two list with softmax probability distribution\n",
    "    interleaved_list = []\n",
    "    relevance_list = []\n",
    "    for _ in range(len(E) + len(P)):\n",
    "        coin = rand.choice([0,1])\n",
    "        if(coin == 0):\n",
    "            # check if E is empty then just sample from P\n",
    "            if not E:\n",
    "                # choose element from P randomly\n",
    "                picked = np.random.choice(P, 1, p=softmax_distribution_P)[0]\n",
    "                # append it to interleaved list\n",
    "                interleaved_list.append((picked, \"P\"))\n",
    "                relevance_list.append(picked)\n",
    "                # remove the picked element\n",
    "                P.remove(picked)\n",
    "                # renormalize the distribution\n",
    "                softmax_distribution_P = softmax(3,len(P))\n",
    "            # else sample from E\n",
    "            else:\n",
    "                # choose element from E randomly\n",
    "                picked = np.random.choice(E, 1, p=softmax_distribution_E)[0]\n",
    "                # append it to interleaved list\n",
    "                interleaved_list.append((picked, \"E\"))\n",
    "                relevance_list.append(picked)\n",
    "                # remove the picked element\n",
    "                E.remove(picked)\n",
    "                # renormalize the distribution\n",
    "                softmax_distribution_E = softmax(3,len(E))\n",
    "        if(coin == 1):\n",
    "            # check if P is empty then just sample from E\n",
    "            if not P:\n",
    "                # choose element from E randomly\n",
    "                picked = np.random.choice(E, 1, p=softmax_distribution_E)[0]\n",
    "                # append it to interleaved list\n",
    "                interleaved_list.append((picked, \"E\"))\n",
    "                relevance_list.append(picked)\n",
    "                # remove the picked element\n",
    "                E.remove(picked)\n",
    "                # renormalize the distribution\n",
    "                softmax_distribution_E = softmax(3,len(E))\n",
    "            # else sample from P\n",
    "            else:\n",
    "                # choose element from P randomly\n",
    "                picked = np.random.choice(P, 1, p=softmax_distribution_P)[0]\n",
    "                # append it to interleaved list\n",
    "                interleaved_list.append((picked, \"P\"))\n",
    "                relevance_list.append(picked)\n",
    "                # remove the picked element\n",
    "                P.remove(picked)\n",
    "                # renormalize the distribution\n",
    "                softmax_distribution_P = softmax(3,len(P))\n",
    "    # generate user clicks randomly\n",
    "    return interleaved_list, relevance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = rand.randint(0,59049-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def who_wins(interleaved_list, clicks):\n",
    "    # calculate scores\n",
    "    E_wins, P_wins = 0, 0\n",
    "    for i, click in enumerate(clicks):\n",
    "        if click == 1:\n",
    "            if interleaved_list[i][1] == \"E\":\n",
    "                E_wins += 1\n",
    "            if interleaved_list[i][1] == \"P\":\n",
    "                P_wins += 1\n",
    "    if E_wins > P_wins:\n",
    "        return \"E win\"\n",
    "    elif E_wins < P_wins:\n",
    "        return \"P Win\"\n",
    "    else:\n",
    "        return \"tie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 0, 2, 2, 1], [1, 1, 1, 2, 0])\n",
      "([(1, 'P'), (0, 'E'), (1, 'P'), (0, 'E'), (2, 'E'), (1, 'P'), (2, 'E'), (2, 'P'), (1, 'E'), (0, 'P')], [1, 0, 1, 0, 2, 1, 2, 2, 1, 0])\n",
      "([(1, 'P'), (0, 'E'), (1, 'P'), (0, 'E'), (1, 'P'), (2, 'P'), (2, 'E'), (2, 'E'), (0, 'P'), (1, 'E')], [1, 0, 1, 0, 1, 2, 2, 2, 0, 1])\n",
      "E win\n",
      "P Win\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "\n",
    "click_test = [1,1,0,0,1,0,0,0,0,0]\n",
    "td_interleaved = team_draft_interleave(pairs[r], False)\n",
    "prob_interleaved = probabilistic_interleave(pairs[r])\n",
    "print pairs[r]\n",
    "print td_interleaved\n",
    "print prob_interleaved\n",
    "print who_wins(td_interleaved[0], click_test)\n",
    "print who_wins(prob_interleaved[0], click_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do probabilistic interleave : \n",
    "- Make it general\n",
    "- Explanation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Explanations\n",
    "\n",
    "\n",
    "##### Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Implement User Clicks Simulation ( 25 points)\n",
    "\n",
    "Having interleaved all the ranking pairs an online experiment could be ran. However, given that we do not\n",
    "have any users (and the entire homework is a big simulation) we will simulate user clicks.<br>\n",
    "We have considered a number of click models including:<br>\n",
    "* 1. Random Click Model (RCM)\n",
    "* 2. Position-Based Model (PBM)\n",
    "* 3. Simple Dependent Click Model (SDCM)\n",
    "* 4. Simple Dynamic Bayesian Network (SDBN)\n",
    "\n",
    "Consider two different click models, (a) the Random Click Model (RCM), and (b) one out of the\n",
    "remaining 3 aforementioned models. The parameters of some of these models can be estimated using the\n",
    "Maximum Likelihood Estimation (MLE) method, while others require using the\n",
    "Expectation-Maximization (EM) method. Implement the two models so that:\n",
    "* (a) there is a method that learns the parameters of the model given a set of training data, \n",
    "* (b) there is a method that predicts the click probability given a ranked list of relevance labels, \n",
    "* (c) there is a method that decides - stochastically -whether a document is clicked based on these probabilities.<br>\n",
    "\n",
    "Having implemented the two click models, estimate the model parameters using the Yandex Click Log\n",
    "\n",
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Random Click Model (RCM) \n",
    "def learns_param_RCM(training_data):\n",
    "    numb_doc = 0\n",
    "    num_click=0\n",
    "    for data in training_data:\n",
    "        #Only if the action is a query, we add to the total number of doc the number of doc returned by the query\n",
    "        if(\"Q\" in data[2]):\n",
    "            for j in range(5,len(data)):\n",
    "                numb_doc+=1\n",
    "        #If the action is a click, we add one to the total number of click \n",
    "        if(\"C\" in data[2]):\n",
    "            num_click+=1\n",
    "    return float(num_click)/numb_doc\n",
    "\n",
    "def predict_click_proba_RCM(training_data,relevance_labels) :\n",
    "    rau = learns_param_RCM(training_data)\n",
    "    #since the probability is rau for every document, we return a list of rau that has the length of the relevance labels list\n",
    "    return [rau]*len(relevance_labels)\n",
    "\n",
    "def is_doc_clicked_RCM(training_data,relevance_labels,document):\n",
    "    proba_distrib = predict_click_proba_RCM(training_data,relevance_labels)\n",
    "    return (bernoulli.rvs(proba_distrib[document])==1)\n",
    "\n",
    "def generate_click_RCM(training_data, interleaved_list):\n",
    "    click = []\n",
    "    proba_distrib = predict_click_proba_RCM(training_data,relevance_labels)\n",
    "    for i, elem in enumerate(proba_distrib):\n",
    "        click.append(bernoulli.rvs(elem))\n",
    "    return click\n",
    "\n",
    "#Simple Dynamic Bayesian Network (SDBN)\n",
    "\n",
    "#This means that the attractiveness of a document, given a query, is directly proportional to the number\n",
    "#of times this document is clicked and inversely proportional to the number of times it is presented in\n",
    "#response to the given query above or at the last-clicked rank.\n",
    "def add1(key,dictionnary):\n",
    "    if key not in dictionnary:\n",
    "        dictionnary[key]=1\n",
    "    else:\n",
    "        dictionnary[key]+=1\n",
    "\n",
    "def learns_param_SBDN(training_data):\n",
    "    #the number of times document u is presented as response for the query q : used to calculate attrativness\n",
    "    doc_presented_q_dict= {}\n",
    "    #the number of times document u is clicked for the query q: used to calculate attrativness and satisfaction\n",
    "    doc_clicked_q_dict= {}\n",
    "    #the number of times document u is the last clicked for the query q: used to calculate satisfaction\n",
    "    doc_last_clicked_q_dict= {}\n",
    "    \n",
    "    global_index = 0 #index used to browser training_data\n",
    "    for session in range(int(training_data[len(training_data)-1][0])):\n",
    "        #index used to browser all the documents in a session\n",
    "        i=0 \n",
    "        end_of_session=False\n",
    "        #memorise the last query id\n",
    "        last_query =-1\n",
    "        #loop to browser the current session\n",
    "        while(not end_of_session):\n",
    "            #the session stop if the next index is not equal to the current session index or we reach the end of the list\n",
    "            if not (global_index+i+1)>= len(training_data):\n",
    "                end_of_session=(not int(training_data[global_index+i+1][0]) == session)\n",
    "            else:\n",
    "                end_of_session=True\n",
    "                \n",
    "            #memorize the current element for efficency reasons\n",
    "            element = training_data[global_index+i]\n",
    "            #memorize the last query\n",
    "            if(element[2]=='Q'):\n",
    "                last_query=int(element[3])\n",
    "                #memorize that a document appears for a query\n",
    "                for j in range(5,len(element)):\n",
    "                    key= (int(element[j]),last_query)\n",
    "                    add1(key,doc_presented_q_dict)\n",
    "            #memorize the click\n",
    "            if(element[2]=='C'):\n",
    "                key= (int(element[3]),last_query)\n",
    "                #If this is the last click of a session: memorize in the last click dictionnary \n",
    "                if end_of_session:\n",
    "                    add1(key,doc_last_clicked_q_dict)\n",
    "                #memorize in the click dictionnary \n",
    "                add1(key,doc_clicked_q_dict)\n",
    "            #increment the index used to browser the session \n",
    "            i+=1\n",
    "        #after the end of a session, skip to the next session\n",
    "        global_index+=i\n",
    "    \n",
    "    attract= {}\n",
    "    satisf = {}\n",
    "    #calculate attractivness which is #u is clicked for query q / #u appears with q \n",
    "    for key in doc_presented_q_dict:\n",
    "        if key in doc_clicked_q_dict:\n",
    "            attract[key]= float(doc_clicked_q_dict[key]) /  doc_presented_q_dict[key]\n",
    "        else: \n",
    "            attract[key]=0\n",
    "    #calculate satisfaction which is #u is last clicked document for query q / #u is clicked for query q \n",
    "    for key in doc_clicked_q_dict:\n",
    "        if key in doc_last_clicked_q_dict:\n",
    "            satisf[key]= float(doc_last_clicked_q_dict[key]) /  doc_clicked_q_dict[key]\n",
    "        else: \n",
    "            satisf[key]=0\n",
    "    return attract,satisf\n",
    "\n",
    "#CHECK WITH REZKA: what about the conditionnal probability of a click ?? \n",
    "#query is the query number \n",
    "def predict_click_proba_SBDN(training_data,relevance_labels,doc_ids,query) :\n",
    "    attract,satisf= learns_param_SBDN(training_data)\n",
    "    proba= [0]* len(relevance_labels)\n",
    "    exam_proba=[0]*len(relevance_labels)\n",
    "    \n",
    "    for i in len(doc_ids):\n",
    "        alpha_i_q = 0\n",
    "        key = (doc_ids[i],query)\n",
    "        if key in attract:\n",
    "            alpha_i_q = attract[key]\n",
    "            \n",
    "        sigma_i_q = 0 \n",
    "        if key in satisf:\n",
    "            sigma_i_q = satisf[key]\n",
    "        \n",
    "        if(i==0):\n",
    "            proba[i]=alpha_i_q\n",
    "            exam_proba[i]=1\n",
    "        else :\n",
    "            proba[i]=alpha_i_q * exam_proba[i-1]\n",
    "            #gamma=1\n",
    "            exam_proba[i]=exam_proba[i-1] * ( (1-sigma_i_q)*alpha_i_q + (1-alpha_i_q) )\n",
    "    return proba\n",
    "\n",
    "#docuement: document position in relevance_labels list \n",
    "def is_doc_clicked_SBDN(training_data,relevance_labels,doc_ids,query,document):\n",
    "    proba_distrib= predict_click_proba_SBDN(training_data,relevance_labels,doc_ids,query)\n",
    "    return (bernoulli.rvs(proba_distrib[document])==1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CHECK WITH REZKA__\n",
    "\n",
    "$P(C_u=1) = \\alpha_{uq} \\epsilon_{r_u} $\n",
    "\n",
    "if $(P(C_u=1)$\n",
    "\n",
    "$\\epsilon_{r+1}= \\epsilon_{r}\\gamma  [(1-\\sigma_{uq})\\alpha_{uq} +(1-\\alpha_{uq})] $\n",
    "\n",
    "else if $ (P(C_u=1|C_{<r_u})$\n",
    "\n",
    "$\\epsilon_{r+1}= c_r^{(s)}\\gamma (1-\\sigma_{u_r q}) + (1-c_r^{(s)}) \\frac{(1-\\alpha_{u_r q}) \\epsilon_r \\gamma}{1-\\alpha_{u_r q}\\epsilon_r}  $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('isDocClicked', False)\n",
      "[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#open file and preprocess data\n",
    "training_data=[]\n",
    "relevance_labels= [2, 1, 2, 1, 1, 0, 2, 1, 2, 2]\n",
    "document= 3\n",
    "\n",
    "f= open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "for line in f:\n",
    "    training_data.append(line.split(\"\\n\")[0].split('\\t'))\n",
    "f.close()\n",
    "\n",
    "print(\"isDocClicked\", is_doc_clicked_RCM(training_data,relevance_labels,document))\n",
    "print generate_click_RCM(training_data, relevance_labels)\n",
    "\n",
    "#print(\"isDocClicked\", is_doc_clicked_SBDN(training_data,relevance_labels,document))\n",
    "#print(learns_param_SBDN(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanations\n",
    "\n",
    "##### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Simulate Interleaving Experiment ( 10 points)\n",
    "\n",
    "Having implemented the click models, it is time to run the simulated experiment.<br>\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and\n",
    "measure the proportion p of wins for E.<br>\n",
    "\n",
    "(Note 7: Some of the models above include an attractiveness parameter $\\alpha_{uq}$ . Use the relevance label to\n",
    "assign this parameter by setting $\\alpha_{uq}$ for a document u in the ranked list accordingly.)\n",
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For our case, we have to redined the attractivness and thus the previous functions of SBDN:\n",
    "#query value is a random number generated by generate_query\n",
    "#doc_ids ...                   generated by generate_doc_ids\n",
    "def specific_learns_param_SBDN(training_data,relevance_labels,doc_ids,query):\n",
    "    attract,satisf= learns_param_SBDN(training_data)\n",
    "    #the previous attractivness is incorrect, we calculate it using relevance labels: \n",
    "    #thanks to formula p47 of \"Click Models for Web Search\",for a query q \n",
    "    \n",
    "    #relevance(u) = satisf(u) * attract(u) => attract(u) = relevance(u) / satisf(u)\n",
    "    relev_attract=[0]*len(relevance_labels)\n",
    "    for i in range(relevance_labels):\n",
    "        satisf_u = 0\n",
    "        key = (doc_ids[i],query)\n",
    "        if key in satisf:\n",
    "            satisf_u = satisf[u]\n",
    "        relev_attract[i]= relevance_labels[i]/satisf_u\n",
    "    return relev_attract,satisf\n",
    "\n",
    "def generate_query(training_data):\n",
    "    query= set()\n",
    "    for data in training_data:\n",
    "        if data[2]==\"Q\":\n",
    "            query.add(int(data[3]))\n",
    "    return rand.sample(query,1)\n",
    "    \n",
    "def generate_doc_ids(training_data,number_ids, number_ids_different):\n",
    "    doc_id_set = set()\n",
    "    res=[]\n",
    "    for data in training_data:\n",
    "        if data[2]==\"Q\":\n",
    "            for j in range(5,len(data)):\n",
    "                doc_id_set.add(int(data[j]))\n",
    "   \n",
    "    for i in range(number_ids):\n",
    "        doc_id = rand.sample(doc_id_set,1)[0]\n",
    "        valid_id = (doc_id not in number_ids_different) and (doc_id not in res)\n",
    "        while (not valid_id):\n",
    "            doc_id = rand.sample(doc_id_set,1)[0]\n",
    "            valid_id = (doc_id not in number_ids_different) and (doc_id not in res)\n",
    "            valid_id= valid_id \n",
    "        res.append(doc_id)\n",
    "    return res \n",
    "        \n",
    "#query value is a random number generated by generate_query\n",
    "#doc_ids ...                   generated by generate_doc_ids\n",
    "def specific_predict_click_proba_SBDN(training_data,relevance_labels):\n",
    "    attract,satisf= specific_learns_param_SBDN(training_data,relevance_labels, query, doc_ids)\n",
    "    \n",
    "    proba= [0]* len(relevance_labels)\n",
    "    exam_proba=[0]*len(relevance_labels)\n",
    "    \n",
    "    for i in len(relevance_labels):\n",
    "        alpha_i_q = attract[i]\n",
    "            \n",
    "        sigma_i_q = 0 \n",
    "        key = (doc_ids[i],query)\n",
    "        if key in satisf [key]:\n",
    "            sigma_i_q = satisf[key]\n",
    "        \n",
    "        if(i==0):\n",
    "            proba[i]=alpha_i_q\n",
    "            exam_proba[i]=1\n",
    "        else :\n",
    "            proba[i]=alpha_i_q * exam_proba[i-1]\n",
    "            #gamma=1\n",
    "            exam_proba[i]=exam_proba[i-1] * ( (1-sigma_i_q)*alpha_i_q + (1-alpha_i_q) )\n",
    "    return proba\n",
    "#query value is a random number generated by generate_query\n",
    "#doc_ids ...                   generated by generate_doc_ids\n",
    "def specific_is_doc_clicked_SBDN(training_data,relevance_labels,doc_ids,query,document):\n",
    "    proba_distrib= specific_predict_click_proba_SBDN(training_data,relevance_labels,doc_ids,query)\n",
    "    return (bernoulli.rvs(proba_distrib[document])==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pairs = rand.sample(pairs, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.460317460317\n",
      "0.403846153846\n"
     ]
    }
   ],
   "source": [
    "E_wins_proportion_td = 0\n",
    "E_wins_proportion_prob = 0\n",
    "tie_td = 0\n",
    "tie_prob = 0\n",
    "for i, elem in enumerate(test_pairs):\n",
    "    team_draft_res = team_draft_interleave(elem, False)\n",
    "    prob_res = probabilistic_interleave(elem)\n",
    "    clicks_td_RCM = generate_click_RCM(training_data, team_draft_res[1])\n",
    "    clicks_prob_RCM = generate_click_RCM(training_data, prob_res[1])\n",
    "    if(who_wins(team_draft_res[0], clicks_td_RCM) == \"E win\"):\n",
    "        E_wins_proportion_td += 1\n",
    "    elif(who_wins(team_draft_res[0], clicks_td_RCM) == \"tie\"):\n",
    "        tie_td += 1\n",
    "    if(who_wins(prob_res[0], clicks_prob_RCM) == \"E win\"):\n",
    "        E_wins_proportion_prob += 1\n",
    "    elif(who_wins(prob_res[0], clicks_prob_RCM) == \"tie\"):\n",
    "        tie_prob += 1\n",
    "\n",
    "print float(E_wins_proportion_td) / (100.0 - tie_td)\n",
    "print float(E_wins_proportion_prob) / (100.0 - tie_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To do :\n",
    "- More verbality\n",
    "- SDBN simulation\n",
    "- Explanation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
