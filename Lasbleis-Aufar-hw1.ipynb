{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 Code and Report\n",
    "AUFAR Rezka and LASBLEIS Alexandre "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the first step you will generate pair s of rankings of relevance, for the production P and experimental E,\n",
    "respectively, for a hypothetical query q . Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all\n",
    "possible P and E ranking pairs of length 5, for which E outperforms P.\n",
    "\n",
    "Example:<br>\n",
    "P: {N N N N N}<br>\n",
    "E: {N N N N R}<br>\n",
    "… <br>\n",
    "P: {HR HR HR HR R}<br>\n",
    "E: {HR HR HR HR HR}<br>\n",
    "\n",
    "(Note 1: If you do not have enough computational power, sample 1000 pair uniformly at random to show\n",
    "your work.)\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert int to relevance\n",
    "#_list : list of integer to be converted\n",
    "#return : list of string where every integer has been replaced by its relevance interpretation\n",
    "def to_string(_list): \n",
    "    L=[]\n",
    "    for i in _list:\n",
    "        if i==0:\n",
    "            L.append(\"N \")\n",
    "        elif i==1:\n",
    "            L.append(\"R \")\n",
    "        elif i==2:\n",
    "            L.append(\"HR\")\n",
    "        else: \n",
    "            L.append(\"? \")\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs = [] #variable used to store all pairs \n",
    "Ps = []\n",
    "Es = []\n",
    "\n",
    "#Create all combinaison for P, there are 3^5 = 243 possibilities since P is of length 5 and each member have 3 possible values:\n",
    "# 0 (representing \"N\" ), 1 (representing \"R), 2 (representing \"HR)\n",
    "for p_possibility in range(243): \n",
    "    P=[None]*5\n",
    "    #create all five values for P \n",
    "    for p_pos in range(5):\n",
    "        P[p_pos]= p_possibility // (np.power(3,p_pos)) %3 #formula to create all combinaison\n",
    "    #we do the same for E    \n",
    "    for e_possibility in range(243): \n",
    "        E=[None]*5\n",
    "        for e_pos in range(5):\n",
    "            E[e_pos]= e_possibility // (np.power(3,e_pos)) %3 #formula to create all combinaison\n",
    "        pairs.append((P,E)) #adding the pair since every pair is valid\n",
    "        #Memorize all created Es but only once\n",
    "        if p_possibility==0:\n",
    "            Es.append(E)\n",
    "        #memorize all created Ps\n",
    "    Ps.append(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59049\n",
      "('P: ', ['N ', 'HR', 'R ', 'HR', 'HR'], 'E: ', ['R ', 'R ', 'N ', 'R ', 'R '])\n",
      "243\n",
      "[[0, 0, 0, 0, 0], [1, 0, 0, 0, 0], [2, 0, 0, 0, 0], [0, 1, 0, 0, 0], [1, 1, 0, 0, 0], [2, 1, 0, 0, 0], [0, 2, 0, 0, 0], [1, 2, 0, 0, 0], [2, 2, 0, 0, 0], [0, 0, 1, 0, 0], [1, 0, 1, 0, 0], [2, 0, 1, 0, 0], [0, 1, 1, 0, 0], [1, 1, 1, 0, 0], [2, 1, 1, 0, 0], [0, 2, 1, 0, 0], [1, 2, 1, 0, 0], [2, 2, 1, 0, 0], [0, 0, 2, 0, 0], [1, 0, 2, 0, 0], [2, 0, 2, 0, 0], [0, 1, 2, 0, 0], [1, 1, 2, 0, 0], [2, 1, 2, 0, 0], [0, 2, 2, 0, 0], [1, 2, 2, 0, 0], [2, 2, 2, 0, 0], [0, 0, 0, 1, 0], [1, 0, 0, 1, 0], [2, 0, 0, 1, 0], [0, 1, 0, 1, 0], [1, 1, 0, 1, 0], [2, 1, 0, 1, 0], [0, 2, 0, 1, 0], [1, 2, 0, 1, 0], [2, 2, 0, 1, 0], [0, 0, 1, 1, 0], [1, 0, 1, 1, 0], [2, 0, 1, 1, 0], [0, 1, 1, 1, 0], [1, 1, 1, 1, 0], [2, 1, 1, 1, 0], [0, 2, 1, 1, 0], [1, 2, 1, 1, 0], [2, 2, 1, 1, 0], [0, 0, 2, 1, 0], [1, 0, 2, 1, 0], [2, 0, 2, 1, 0], [0, 1, 2, 1, 0], [1, 1, 2, 1, 0], [2, 1, 2, 1, 0], [0, 2, 2, 1, 0], [1, 2, 2, 1, 0], [2, 2, 2, 1, 0], [0, 0, 0, 2, 0], [1, 0, 0, 2, 0], [2, 0, 0, 2, 0], [0, 1, 0, 2, 0], [1, 1, 0, 2, 0], [2, 1, 0, 2, 0], [0, 2, 0, 2, 0], [1, 2, 0, 2, 0], [2, 2, 0, 2, 0], [0, 0, 1, 2, 0], [1, 0, 1, 2, 0], [2, 0, 1, 2, 0], [0, 1, 1, 2, 0], [1, 1, 1, 2, 0], [2, 1, 1, 2, 0], [0, 2, 1, 2, 0], [1, 2, 1, 2, 0], [2, 2, 1, 2, 0], [0, 0, 2, 2, 0], [1, 0, 2, 2, 0], [2, 0, 2, 2, 0], [0, 1, 2, 2, 0], [1, 1, 2, 2, 0], [2, 1, 2, 2, 0], [0, 2, 2, 2, 0], [1, 2, 2, 2, 0], [2, 2, 2, 2, 0], [0, 0, 0, 0, 1], [1, 0, 0, 0, 1], [2, 0, 0, 0, 1], [0, 1, 0, 0, 1], [1, 1, 0, 0, 1], [2, 1, 0, 0, 1], [0, 2, 0, 0, 1], [1, 2, 0, 0, 1], [2, 2, 0, 0, 1], [0, 0, 1, 0, 1], [1, 0, 1, 0, 1], [2, 0, 1, 0, 1], [0, 1, 1, 0, 1], [1, 1, 1, 0, 1], [2, 1, 1, 0, 1], [0, 2, 1, 0, 1], [1, 2, 1, 0, 1], [2, 2, 1, 0, 1], [0, 0, 2, 0, 1], [1, 0, 2, 0, 1], [2, 0, 2, 0, 1], [0, 1, 2, 0, 1], [1, 1, 2, 0, 1], [2, 1, 2, 0, 1], [0, 2, 2, 0, 1], [1, 2, 2, 0, 1], [2, 2, 2, 0, 1], [0, 0, 0, 1, 1], [1, 0, 0, 1, 1], [2, 0, 0, 1, 1], [0, 1, 0, 1, 1], [1, 1, 0, 1, 1], [2, 1, 0, 1, 1], [0, 2, 0, 1, 1], [1, 2, 0, 1, 1], [2, 2, 0, 1, 1], [0, 0, 1, 1, 1], [1, 0, 1, 1, 1], [2, 0, 1, 1, 1], [0, 1, 1, 1, 1], [1, 1, 1, 1, 1], [2, 1, 1, 1, 1], [0, 2, 1, 1, 1], [1, 2, 1, 1, 1], [2, 2, 1, 1, 1], [0, 0, 2, 1, 1], [1, 0, 2, 1, 1], [2, 0, 2, 1, 1], [0, 1, 2, 1, 1], [1, 1, 2, 1, 1], [2, 1, 2, 1, 1], [0, 2, 2, 1, 1], [1, 2, 2, 1, 1], [2, 2, 2, 1, 1], [0, 0, 0, 2, 1], [1, 0, 0, 2, 1], [2, 0, 0, 2, 1], [0, 1, 0, 2, 1], [1, 1, 0, 2, 1], [2, 1, 0, 2, 1], [0, 2, 0, 2, 1], [1, 2, 0, 2, 1], [2, 2, 0, 2, 1], [0, 0, 1, 2, 1], [1, 0, 1, 2, 1], [2, 0, 1, 2, 1], [0, 1, 1, 2, 1], [1, 1, 1, 2, 1], [2, 1, 1, 2, 1], [0, 2, 1, 2, 1], [1, 2, 1, 2, 1], [2, 2, 1, 2, 1], [0, 0, 2, 2, 1], [1, 0, 2, 2, 1], [2, 0, 2, 2, 1], [0, 1, 2, 2, 1], [1, 1, 2, 2, 1], [2, 1, 2, 2, 1], [0, 2, 2, 2, 1], [1, 2, 2, 2, 1], [2, 2, 2, 2, 1], [0, 0, 0, 0, 2], [1, 0, 0, 0, 2], [2, 0, 0, 0, 2], [0, 1, 0, 0, 2], [1, 1, 0, 0, 2], [2, 1, 0, 0, 2], [0, 2, 0, 0, 2], [1, 2, 0, 0, 2], [2, 2, 0, 0, 2], [0, 0, 1, 0, 2], [1, 0, 1, 0, 2], [2, 0, 1, 0, 2], [0, 1, 1, 0, 2], [1, 1, 1, 0, 2], [2, 1, 1, 0, 2], [0, 2, 1, 0, 2], [1, 2, 1, 0, 2], [2, 2, 1, 0, 2], [0, 0, 2, 0, 2], [1, 0, 2, 0, 2], [2, 0, 2, 0, 2], [0, 1, 2, 0, 2], [1, 1, 2, 0, 2], [2, 1, 2, 0, 2], [0, 2, 2, 0, 2], [1, 2, 2, 0, 2], [2, 2, 2, 0, 2], [0, 0, 0, 1, 2], [1, 0, 0, 1, 2], [2, 0, 0, 1, 2], [0, 1, 0, 1, 2], [1, 1, 0, 1, 2], [2, 1, 0, 1, 2], [0, 2, 0, 1, 2], [1, 2, 0, 1, 2], [2, 2, 0, 1, 2], [0, 0, 1, 1, 2], [1, 0, 1, 1, 2], [2, 0, 1, 1, 2], [0, 1, 1, 1, 2], [1, 1, 1, 1, 2], [2, 1, 1, 1, 2], [0, 2, 1, 1, 2], [1, 2, 1, 1, 2], [2, 2, 1, 1, 2], [0, 0, 2, 1, 2], [1, 0, 2, 1, 2], [2, 0, 2, 1, 2], [0, 1, 2, 1, 2], [1, 1, 2, 1, 2], [2, 1, 2, 1, 2], [0, 2, 2, 1, 2], [1, 2, 2, 1, 2], [2, 2, 2, 1, 2], [0, 0, 0, 2, 2], [1, 0, 0, 2, 2], [2, 0, 0, 2, 2], [0, 1, 0, 2, 2], [1, 1, 0, 2, 2], [2, 1, 0, 2, 2], [0, 2, 0, 2, 2], [1, 2, 0, 2, 2], [2, 2, 0, 2, 2], [0, 0, 1, 2, 2], [1, 0, 1, 2, 2], [2, 0, 1, 2, 2], [0, 1, 1, 2, 2], [1, 1, 1, 2, 2], [2, 1, 1, 2, 2], [0, 2, 1, 2, 2], [1, 2, 1, 2, 2], [2, 2, 1, 2, 2], [0, 0, 2, 2, 2], [1, 0, 2, 2, 2], [2, 0, 2, 2, 2], [0, 1, 2, 2, 2], [1, 1, 2, 2, 2], [2, 1, 2, 2, 2], [0, 2, 2, 2, 2], [1, 2, 2, 2, 2], [2, 2, 2, 2, 2]]\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "print(len(pairs)) #= 59049 which is equal to the expected length = 3 ^10 = 59049\n",
    "#print(pairs) #print all pairs but this takes a lot of place\n",
    "\n",
    "#print a random pair to show that it makes sense \n",
    "r = rand.randint(0,59049-1)\n",
    "print(\"P: \" , to_string(pairs[r][0]), \"E: \",to_string(pairs[r][1]))\n",
    "\n",
    "#check if Es is as expected \n",
    "print(len(Es))#expect 243 = 3^5 \n",
    "print(Es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanations\n",
    "In order to contruct all possible pairs of P and E with a 3-graded relevance {N, R, HR}, we first encode those relevance as integers: <br>\n",
    "* 0 for N <br>\n",
    "* 1 for R <br>\n",
    "* 2 for HR <br>\n",
    "This encoding allows us to do easy comparison between elements.\n",
    "Then we created all pairs using this formula : $value = \\frac{possibility}{3^{pos}} [3]$ with $possibility \\in \\{0,...,243\\}$ and $ pos \\in \\{0,1,2,3,4\\}$.\n",
    "\n",
    "This give the following sequence depending on the position of the element in the list: <br>\n",
    "* 0: $\\{0,1,2,0,1,2,0,1,2,0,1,2...\\}$\n",
    "* 1: $\\{0,0,0,1,1,1,2,2,2,0,0,0...\\}$\n",
    "* 2: $\\{0,0,0,0,0,0,0,0,0,1,1,1,...\\}$\n",
    "* ... and so on\n",
    "\n",
    "This formula is then used for the production P and the experiment E. \n",
    "\n",
    "##### Analysis\n",
    "According to the formula we showed before, we should find all the possible combination. Futhermore, the number of pairs found is equal to the one we expected $3^{10}$ and the number of E and P are $3^{5}$ each which is also what is expected. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 2 : Implement Evaluation Measures (15 points)\n",
    "\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above.\n",
    "\n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant\n",
    "documents in the entire collection – pay extra attention on how to find this)\n",
    "\n",
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Remove for evaluation \n",
    "\n",
    "#Recall at rank k \n",
    "def Binary_measure_recal(test_set, r):\n",
    "    num_test_rel = 0 \n",
    "    total_num_rel=0\n",
    "    for i in range(len(test_set)):\n",
    "        if test_set[i]>0:\n",
    "            if i<r:\n",
    "                num_test_rel+=1\n",
    "            total_num_rel+=1\n",
    "    return float(num_test_rel) / total_num_rel\n",
    "\n",
    "#Average precision at rank k \n",
    "def Binary_measure_avg_precision(test_set, r):\n",
    "    num_rel=0\n",
    "    sum_precision = 0 \n",
    "    total_num_rel=0\n",
    "    temp=\"(\"\n",
    "    for i in range(len(test_set)):\n",
    "        if test_set[i]>0:\n",
    "            if i<r:\n",
    "                num_rel+=1\n",
    "                temp+=\" \"+str(num_rel)+\"/\"+str(i+1)+\"+\"\n",
    "                sum_precision+=float(num_rel)/(i+1)\n",
    "            total_num_rel+=1\n",
    "    temp=temp[:len(temp)-1]+\")\"\n",
    "    #print(temp,\"/\",total_num_rel)\n",
    "    return float(sum_precision) / total_num_rel\n",
    "#Normalized discount cumulative gain nDCG at rank r\n",
    "def nDCG(test_set, r):\n",
    "    orderedSet= list(test_set)\n",
    "    orderedSet.sort(reverse=True)\n",
    "    return DCG(test_set, r) / DCG(orderedSet, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BINARY \n",
    "#Precision at rank r \n",
    "#test_set: the collection to be tested \n",
    "#r: the rank for which we test the collection \n",
    "#return : the precision \n",
    "def Binary_measure_precision(test_set, r):\n",
    "    assert r <= len(test_set)\n",
    "    assert r!= 0\n",
    "    num_test_rel = 0 \n",
    "    for i in range(r):\n",
    "        #According to our notation, if test_set[i]>0, the document is relevant\n",
    "        if test_set[i]>0:\n",
    "            num_test_rel+=1\n",
    "    return float(num_test_rel) / r #precision is number of relevant doc/ number of doc considered\n",
    "\n",
    "#MULTI GRADED \n",
    "#Discount cumulative gain DCG at rank r\n",
    "#test_set: the collection to be tested \n",
    "#r: the rank for which we test the collection \n",
    "#return : the gain \n",
    "def DCG(test_set, r):\n",
    "    assert r <= len(test_set)\n",
    "    assert r!= 0\n",
    "    res=0\n",
    "    for k in range(1,r+1):#r is the rank so it starts at 1\n",
    "        rel_r = test_set[k-1]#index of element = rank -1 since rank starts at 1 and index at 0\n",
    "        res+=float(np.power(2,rel_r)-1)/(np.log2(1+k))\n",
    "    return res\n",
    "\n",
    "#Rank Biased Precision w ith persistence parameter 𝜃 =0.8\n",
    "#test_set: the collection to be tested \n",
    "#theta: the theta parameter of the RBP formula\n",
    "#return : the precision\n",
    "def RBP (test_set,theta=0.8):\n",
    "    res=0\n",
    "    for k in range(1,len(test_set)+1):\n",
    "        rel_k=test_set[k-1]\n",
    "        res+= rel_k * np.power(theta,k-1) * (1.0-theta)\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific ordering\n",
      "('Rank = ', 1, 'Precision', 1.0, 'DCG', 1.0, 'RBP', 1.06)\n",
      "('Rank = ', 2, 'Precision', 0.5, 'DCG', 1.0, 'RBP', 1.06)\n",
      "('Rank = ', 3, 'Precision', 0.667, 'DCG', 2.5, 'RBP', 1.06)\n",
      "('Rank = ', 4, 'Precision', 0.75, 'DCG', 15.851, 'RBP', 1.06)\n",
      "('Rank = ', 5, 'Precision', 0.6, 'DCG', 15.851, 'RBP', 1.06)\n",
      "('Rank = ', 6, 'Precision', 0.667, 'DCG', 16.207, 'RBP', 1.06)\n",
      "('Rank = ', 7, 'Precision', 0.571, 'DCG', 16.207, 'RBP', 1.06)\n",
      "('Rank = ', 8, 'Precision', 0.5, 'DCG', 16.207, 'RBP', 1.06)\n",
      "('Rank = ', 9, 'Precision', 0.444, 'DCG', 16.207, 'RBP', 1.06)\n",
      "('Rank = ', 10, 'Precision', 0.5, 'DCG', 16.496, 'RBP', 1.06)\n",
      "\n",
      "Perfect ordering\n",
      "('Rank = ', 1, 'Precision', 1.0, 'DCG', 31.0, 'RBP', 1.632)\n",
      "('Rank = ', 2, 'Precision', 1.0, 'DCG', 32.893, 'RBP', 1.632)\n",
      "('Rank = ', 3, 'Precision', 1.0, 'DCG', 33.393, 'RBP', 1.632)\n",
      "('Rank = ', 4, 'Precision', 1.0, 'DCG', 33.823, 'RBP', 1.632)\n",
      "('Rank = ', 5, 'Precision', 1.0, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 6, 'Precision', 0.833, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 7, 'Precision', 0.714, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 8, 'Precision', 0.625, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 9, 'Precision', 0.556, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 10, 'Precision', 0.5, 'DCG', 34.21, 'RBP', 1.632)\n",
      "\n",
      "Worst ordering\n",
      "('Rank = ', 1, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 2, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 3, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 4, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 5, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 6, 'Precision', 0.167, 'DCG', 0.356, 'RBP', 0.361)\n",
      "('Rank = ', 7, 'Precision', 0.286, 'DCG', 0.69, 'RBP', 0.361)\n",
      "('Rank = ', 8, 'Precision', 0.375, 'DCG', 1.005, 'RBP', 0.361)\n",
      "('Rank = ', 9, 'Precision', 0.444, 'DCG', 1.908, 'RBP', 0.361)\n",
      "('Rank = ', 10, 'Precision', 0.5, 'DCG', 10.869, 'RBP', 0.361)\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "test=        [1,0,2,5,0,1,0,0,0,1]#length 10\n",
    "test_perfect=[5,2,1,1,1,0,0,0,0,0]\n",
    "test_worst=  [0,0,0,0,0,1,1,1,2,5]\n",
    "\n",
    "#The following results are rounded in order to make them readable\n",
    "#Specific ordering \n",
    "print(\"Specific ordering\")\n",
    "for r in range(1,len(test)+1):\n",
    "    print('Rank = ' , r,'Precision',round(Binary_measure_precision(test,r),3),'DCG', round(DCG(test,r),3),\n",
    "          'RBP', round(RBP(test),3))\n",
    "\n",
    "#Perfect ordering \n",
    "print(\"\\nPerfect ordering\")\n",
    "for r in range(1,len(test_perfect)+1):\n",
    "    print('Rank = ' , r,'Precision',round(Binary_measure_precision(test_perfect,r),3),'DCG', round(DCG(test_perfect,r),3),\n",
    "          'RBP', round(RBP(test_perfect),3))\n",
    "\n",
    "#Worst ordering \n",
    "print(\"\\nWorst ordering\")\n",
    "for r in range(1,len(test_worst)+1):\n",
    "    print('Rank = ' , r,'Precision',round(Binary_measure_precision(test_worst,r),3),'DCG', round(DCG(test_worst,r),3),\n",
    "          'RBP', round(RBP(test_worst),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanations\n",
    "For Precision and DCG, we need to ensure that rank k does not exceed the documents length, and k cannot be zero.<br>\n",
    "For DCG and RBP, we also decided that the value of the relevant would be equal to the value at that rank in the list. For example if an example is extremely relevant, he might get a value of 5 (as in the example) which leads to higher gain in DCG than a document which is only highly relevant (value of 2).<br>\n",
    "Otherwise our implementation follows the definition of the method. \n",
    "\n",
    "##### Analysis\n",
    "The result for Precision is highly dependant on the number of non-relevant in the cut-off collection. As k increases, the more non-relevant exists in the cut-off, the lower the precision will be. If we consider a rank equal to the length of the list, the precision is the same whatever the order is. Precision does not that into account how good the relevance of a document is, it only cares if a document is relevant or not. \n",
    "\n",
    "For DCG, the result depends on the degree of the relevance and the order of it. If the highest relevant score exists in the first position of the cut-off, the higher DCG will be. We showed that by displaying the DCG score with perfect ordering which is higher at any rank than the \"specific ordering\". Those value may be hard to use to compare different collections due to the fact that it is sensitive to the degree of relevance. A way to cope with that problem is to use its alternative version: the nDCG (normalized version). \n",
    "\n",
    "For RBP, we disregard rank and therefore we have the same result no matter the rank is. This measure pays great attention to the ordering as shown above: the best score is obtained with a perfect ordering whereas the worst one is obtained with the worst ordering. The model behind RBP considers that when you see a document, you have a probability to see the next document otherwise you stop. This model can be improved by taking into account how relevant is the document that we are looking at. Depending on this relevance, there is a probability to see the next document or to stop. This second model is called ERR model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Calculate the measure (5 points)\n",
    "\n",
    "For the three measures and all P and E ranking pairs constructed above calculate the difference: 𝛥measure\n",
    "= measure E -measure P . Consider only those pairs for which E outperforms P.\n",
    "\n",
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluate if E outperfoms P with respect to a specific measure. \n",
    "#methond_name: the measure used to test. It can be \"precision\", \"DCG\" or \"RBP\"\n",
    "#E: \n",
    "#P:\n",
    "#rank: \n",
    "#return : (outperforms, delta measure) where outperforms is True if E outperfors P with respect to the measure method for the\n",
    "#rank and delta measure is the difference between to measure of E and P for the measure method.\n",
    "def outperforms(methond_name,E,P,rank):\n",
    "    if methond_name==\"precision\":\n",
    "        delta = Binary_measure_precision(E,rank)-Binary_measure_precision(P,rank)\n",
    "        return (delta>0), delta \n",
    "    elif methond_name==\"DCG\":\n",
    "        delta=DCG(E,rank)-DCG(P,rank)\n",
    "        return (delta>0), delta \n",
    "    elif methond_name==\"RBP\":\n",
    "        delta=RBP(E)-RBP(P)\n",
    "        return (delta>0), delta \n",
    "\n",
    "#variable for precision measure \n",
    "outperf_pairs_prec=[] #memorize pairs for which E outperfoms P\n",
    "pairs_set_prec = [] \n",
    "d_measure_prec=[] #memorize \"measure(E)-measure(P)\" for which E outperfoms P\n",
    "\n",
    "#variable for DCG measure \n",
    "outperf_pairs_DCG=[] #memorize pairs for which E outperfoms P\n",
    "pairs_set_DCG = []\n",
    "d_measure_DCG=[] #memorize \"measure(E)-measure(P)\" for which E outperfoms P\n",
    "\n",
    "#variable for RBP measure\n",
    "outperf_pairs_RBP=[] #memorize pairs for which E outperfoms P\n",
    "pairs_set_RBP = []\n",
    "d_measure_RBP=[] #memorize \"measure(E)-measure(P)\" for which E outperfoms P\n",
    "\n",
    "rank = 4\n",
    "for i in range(len(pairs)):#pairs made of (P,E)\n",
    "    #test if E outperforms P with respect to a specific method\n",
    "    outperforms_prec,delta_prec= outperforms(\"precision\",pairs[i][1],pairs[i][0],rank)\n",
    "    outperforms_DCG,delta_DCG= outperforms(\"DCG\",pairs[i][1],pairs[i][0],rank)\n",
    "    outperforms_RBP,delta_RBP= outperforms(\"RBP\",pairs[i][1],pairs[i][0],rank)\n",
    "\n",
    "    #Memorize some \"precision\" information only if E outperfom P\n",
    "    if(outperforms_prec):\n",
    "        outperf_pairs_prec.append(pairs[i])\n",
    "        temp = pairs[i][1] + pairs[i][0]\n",
    "        pairs_set_prec.append(''.join(str(x) for x in temp))\n",
    "        d_measure_prec.append(delta_prec)\n",
    "    \n",
    "    #Memorize some \"DCG\" information only if E outperfom P\n",
    "    if(outperforms_DCG):\n",
    "        outperf_pairs_DCG.append(pairs[i])\n",
    "        temp = pairs[i][1] + pairs[i][0]\n",
    "        pairs_set_DCG.append(''.join(str(x) for x in temp))\n",
    "        d_measure_DCG.append(delta_DCG)\n",
    "    \n",
    "    #Memorize some \"RBP\" information only if E outperfom P\n",
    "    if(outperforms_RBP):\n",
    "        outperf_pairs_RBP.append(pairs[i])\n",
    "        temp = pairs[i][1] + pairs[i][0]\n",
    "        pairs_set_RBP.append(''.join(str(x) for x in temp))\n",
    "        d_measure_RBP.append(delta_RBP)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs for which E outperforms P for all three measures:  17184\n",
      "Pair in precision and in DCG: ['1102100122']\n",
      "Pair in precision and not in DCG: ['1111222102']\n",
      "Pair in DCG and not in precision: ['2222212111']\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "\n",
    "print (\"Number of pairs for which E outperforms P for all three measures: \",len(set.intersection(set(pairs_set_prec), set(pairs_set_DCG), set(pairs_set_RBP))))\n",
    "print (\"Pair in precision and in DCG:\",rand.sample(set.intersection(set(pairs_set_prec), set(pairs_set_DCG)),1))\n",
    "\n",
    "# pick result that is in prec but is not in DCG\n",
    "print (\"Pair in precision and not in DCG:\",rand.sample(set(pairs_set_prec) - set(pairs_set_DCG),1))\n",
    "# pick result that is in DCG but is not in prec\n",
    "print (\"Pair in DCG and not in precision:\",rand.sample(set(pairs_set_DCG) - set(pairs_set_prec),1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pair in precision and not in DCG:', [[1, 1, 1, 1, 2], [2, 2, 1, 0, 2]], 'precision E:', 1.0, 'precision P:', 0.75, 'DCG E: ', 2.5616063116448506, 'DCG P: ', 5.3927892607143724)\n",
      "('Pair in DCG and not in precision:', [[2, 2, 2, 2, 2], [1, 2, 1, 1, 1]], 'DCG E: ', 7.6848189349345519, 'DCG P: ', 3.8234658187877653, 'precision E:', 1.0, 'precision P:', 1.0)\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "pair_prec_not_DCG = [[1,1,1,1,2],[2,2,1,0,2]] # pair E, P \n",
    "pair_DCG_not_prec = [[2,2,2,2,2],[1,2,1,1,1]] # pair E, P \n",
    "print(\"Pair in precision and not in DCG:\",pair_prec_not_DCG, \"precision E:\",\n",
    "      Binary_measure_precision(pair_prec_not_DCG[0],rank),\"precision P:\",\n",
    "      Binary_measure_precision(pair_prec_not_DCG[1],rank),\"DCG E: \",\n",
    "     DCG(pair_prec_not_DCG[0],rank),\"DCG P: \",\n",
    "     DCG(pair_prec_not_DCG[1],rank) )\n",
    "\n",
    "print(\"Pair in DCG and not in precision:\",pair_DCG_not_prec, \"DCG E: \",\n",
    "     DCG(pair_DCG_not_prec[0],rank),\"DCG P: \",\n",
    "     DCG(pair_DCG_not_prec[1],rank),\"precision E:\",\n",
    "      Binary_measure_precision(pair_DCG_not_prec[0],rank),\"precision P:\",\n",
    "      Binary_measure_precision(pair_DCG_not_prec[1],rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanations\n",
    "For precision, DCG, and RBP, we calculate the difference between all combination of E and P that we have generated in Step 1, then we append every difference that is greater than zero, which means that E outperforms P.\n",
    "##### Analysis\n",
    "For each three measures, we have different pairs in which E outperforms P. There are 17184 pairs where E outperforms P in all three measures. However, there exists some pairs where E outperforms P for one kind of measure, but not the other. <br> \n",
    "For example, E=[1,1,1,1,2] outperforms P=[2,2,1,0,2] in precision (delta of 0.25), but not in DCG (delta of -2.83118294971) . This is due to the fact that precision does not check order and the degree of relevance. The same happens the opposite way : E=[2,2,2,2,2] outperforms P=[1,2,1,1,1] in DCG (delta of 3.86135311614), but not in precision (delta of 0). <br>\n",
    "This indicates that in real world application, it is important to implement different kinds of measure so we will have broader feedback to optimize the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 4 : Implement Interleave (15 points)\n",
    "\n",
    "Implement 2 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, AND (2),\n",
    "Probabilistic Interleaving. The interleaving algorithms should (a) given two rankings of relevance\n",
    "interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign\n",
    "credit to the algorithms that produced the rankings.\n",
    "\n",
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assignment assumption : E and P always return different documents\n",
    "# general parameter : consider two pairs can contain same documents \n",
    "\n",
    "def team_draft_interleave(ranking_pairs, general=False):\n",
    "    # interleave two list\n",
    "    interleaved_list = []\n",
    "    relevance_list = []\n",
    "    added_list = []\n",
    "    first_index=0\n",
    "    second_index=0\n",
    "    while((not first_index==len(ranking_pairs[0])) or (not second_index==len(ranking_pairs[1]))):\n",
    "        coin = rand.choice([0,1])\n",
    "        first_link = \"E\" if (coin==0) else \"P\"\n",
    "        second_link = \"P\" if (coin==0) else \"E\"\n",
    "        \n",
    "        coin_first = first_index if (coin==0) else second_index\n",
    "        coin_second = second_index if (coin==0) else first_index\n",
    "        \n",
    "        if(general):\n",
    "            while ((not coin_first==len(ranking_pairs[coin])) and (ranking_pairs[coin][coin_first] in added_list)):\n",
    "                coin_first+=1\n",
    "            if (not coin_first==len(ranking_pairs[coin])):    \n",
    "                interleaved_list.append((ranking_pairs[coin][coin_first], first_link))\n",
    "                added_list.append(ranking_pairs[coin][coin_first])\n",
    "            \n",
    "            while ((not coin_second==len(ranking_pairs[1-coin])) and (ranking_pairs[1-coin][coin_second] in added_list)):\n",
    "                coin_second+=1\n",
    "            \n",
    "            if (not coin_second==len(ranking_pairs[1-coin])):    \n",
    "                interleaved_list.append((ranking_pairs[1-coin][coin_second],second_link))\n",
    "                added_list.append(ranking_pairs[1-coin][coin_second])\n",
    "          \n",
    "        if(not general):\n",
    "            interleaved_list.append((ranking_pairs[coin][first_index], first_link))\n",
    "            interleaved_list.append((ranking_pairs[1-coin][second_index], second_link))\n",
    "            relevance_list.append(ranking_pairs[coin][first_index])\n",
    "            relevance_list.append(ranking_pairs[1-coin][second_index])\n",
    "            \n",
    "        first_index+=1\n",
    "        second_index+=1\n",
    "\n",
    "    return interleaved_list, relevance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(1, 'E'), (2, 'P'), (8, 'P'), (3, 'E'), (4, 'E'), (5, 'E')], [])\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "# tested with general case, which E and P can contain same link\n",
    "general_test = ([1,2,3,4,5], [1,2,8,1,2])\n",
    "print (team_draft_interleave(general_test, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'interleaving'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-932f2ef4b876>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;31m# tested the result with existing library (https://github.com/mpkato/interleaving/tree/master/interleaving)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0minterleaving\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Ranking 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Ranking 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'interleaving'"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "# tested the result with existing library (https://github.com/mpkato/interleaving/tree/master/interleaving)\n",
    "\n",
    "import interleaving\n",
    "a = [1, 2, 3, 4, 5] # Ranking 1\n",
    "b = [6, 7, 8, 9, 10] # Ranking 2\n",
    "method = interleaving.Probabilistic([a, b]) # initialize an interleaving method\n",
    "ranking = method.interleave() # interleaving\n",
    "print (ranking)\n",
    "\n",
    "clicks = [0, 3] # observed clicks, i.e. documents 1 and 2 are clicked\n",
    "result = interleaving.Probabilistic.evaluate(ranking, clicks)\n",
    "print (result) # (0, 1) indicates Ranking 1 won Ranking 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def softmax(tau, length):\n",
    "    total_denominator = 0\n",
    "    softmax_distribution = []\n",
    "    for i in range(1, length+1):\n",
    "        total_denominator += float(1.0/np.power(i,tau))\n",
    "    for i in range(1, length+1):\n",
    "        softmax_distribution.append(float(1.0/np.power(i,tau) / (total_denominator)))\n",
    "    return softmax_distribution\n",
    "\n",
    "def probabilistic_interleave(ranking_pairs):\n",
    "    # generate softmax probability distribution with tau = 3 (Hofmann, 2011)\n",
    "    assert len(ranking_pairs[0]) ==  len(ranking_pairs[1])\n",
    "    E = copy.copy(ranking_pairs[0])\n",
    "    P = copy.copy(ranking_pairs[1])\n",
    "    softmax_distribution_E = softmax(3,len(E))\n",
    "    softmax_distribution_P = softmax(3,len(P))\n",
    "    # interleave two list with softmax probability distribution\n",
    "    interleaved_list = []\n",
    "    relevance_list = []\n",
    "    for _ in range(len(E) + len(P)):\n",
    "        coin = rand.choice([0,1])\n",
    "        if(coin == 0):\n",
    "            # check if E is empty then just sample from P\n",
    "            if not E:\n",
    "                # choose element from P randomly\n",
    "                picked = np.random.choice(P, 1, p=softmax_distribution_P)[0]\n",
    "                # append it to interleaved list\n",
    "                interleaved_list.append((picked, \"P\"))\n",
    "                relevance_list.append(picked)\n",
    "                # remove the picked element\n",
    "                P.remove(picked)\n",
    "                # renormalize the distribution\n",
    "                softmax_distribution_P = softmax(3,len(P))\n",
    "            # else sample from E\n",
    "            else:\n",
    "                # choose element from E randomly\n",
    "                picked = np.random.choice(E, 1, p=softmax_distribution_E)[0]\n",
    "                # append it to interleaved list\n",
    "                interleaved_list.append((picked, \"E\"))\n",
    "                relevance_list.append(picked)\n",
    "                # remove the picked element\n",
    "                E.remove(picked)\n",
    "                # renormalize the distribution\n",
    "                softmax_distribution_E = softmax(3,len(E))\n",
    "        if(coin == 1):\n",
    "            # check if P is empty then just sample from E\n",
    "            if not P:\n",
    "                # choose element from E randomly\n",
    "                picked = np.random.choice(E, 1, p=softmax_distribution_E)[0]\n",
    "                # append it to interleaved list\n",
    "                interleaved_list.append((picked, \"E\"))\n",
    "                relevance_list.append(picked)\n",
    "                # remove the picked element\n",
    "                E.remove(picked)\n",
    "                # renormalize the distribution\n",
    "                softmax_distribution_E = softmax(3,len(E))\n",
    "            # else sample from P\n",
    "            else:\n",
    "                # choose element from P randomly\n",
    "                picked = np.random.choice(P, 1, p=softmax_distribution_P)[0]\n",
    "                # append it to interleaved list\n",
    "                interleaved_list.append((picked, \"P\"))\n",
    "                relevance_list.append(picked)\n",
    "                # remove the picked element\n",
    "                P.remove(picked)\n",
    "                # renormalize the distribution\n",
    "                softmax_distribution_P = softmax(3,len(P))\n",
    "    # generate user clicks randomly\n",
    "    return interleaved_list, relevance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = rand.randint(0,59049-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def who_wins(interleaved_list, clicks):\n",
    "    # calculate scores\n",
    "    E_wins, P_wins = 0, 0\n",
    "    for i, click in enumerate(clicks):\n",
    "        if click == 1:\n",
    "            if interleaved_list[i][1] == \"E\":\n",
    "                E_wins += 1\n",
    "            if interleaved_list[i][1] == \"P\":\n",
    "                P_wins += 1\n",
    "    if E_wins > P_wins:\n",
    "        return \"E win\"\n",
    "    elif E_wins < P_wins:\n",
    "        return \"P Win\"\n",
    "    else:\n",
    "        return \"tie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 0, 2, 2, 1], [2, 1, 0, 1, 2])\n",
      "([(2, 'P'), (0, 'E'), (1, 'P'), (0, 'E'), (2, 'E'), (0, 'P'), (1, 'P'), (2, 'E'), (2, 'P'), (1, 'E')], [2, 0, 1, 0, 2, 0, 1, 2, 2, 1])\n",
      "([(0, 'E'), (2, 'P'), (0, 'E'), (2, 'E'), (2, 'E'), (1, 'P'), (0, 'P'), (1, 'P'), (2, 'P'), (1, 'E')], [0, 2, 0, 2, 2, 1, 0, 1, 2, 1])\n",
      "E win\n",
      "E win\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "\n",
    "click_test = [1,1,0,0,1,0,0,0,0,0]\n",
    "td_interleaved = team_draft_interleave(pairs[r], False)\n",
    "prob_interleaved = probabilistic_interleave(pairs[r])\n",
    "print pairs[r]\n",
    "print td_interleaved\n",
    "print prob_interleaved\n",
    "print who_wins(td_interleaved[0], click_test)\n",
    "print who_wins(prob_interleaved[0], click_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do probabilistic interleave : \n",
    "- Make it general\n",
    "- Explanation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Explanations\n",
    "\n",
    "\n",
    "##### Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Implement User Clicks Simulation ( 25 points)\n",
    "\n",
    "Having interleaved all the ranking pairs an online experiment could be ran. However, given that we do not\n",
    "have any users (and the entire homework is a big simulation) we will simulate user clicks.<br>\n",
    "We have considered a number of click models including:<br>\n",
    "* 1. Random Click Model (RCM)\n",
    "* 2. Position-Based Model (PBM)\n",
    "* 3. Simple Dependent Click Model (SDCM)\n",
    "* 4. Simple Dynamic Bayesian Network (SDBN)\n",
    "\n",
    "Consider two different click models, (a) the Random Click Model (RCM), and (b) one out of the\n",
    "remaining 3 aforementioned models. The parameters of some of these models can be estimated using the\n",
    "Maximum Likelihood Estimation (MLE) method, while others require using the\n",
    "Expectation-Maximization (EM) method. Implement the two models so that:\n",
    "* (a) there is a method that learns the parameters of the model given a set of training data, \n",
    "* (b) there is a method that predicts the click probability given a ranked list of relevance labels, \n",
    "* (c) there is a method that decides - stochastically -whether a document is clicked based on these probabilities.<br>\n",
    "\n",
    "Having implemented the two click models, estimate the model parameters using the Yandex Click Log\n",
    "\n",
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Random Click Model (RCM) \n",
    "\n",
    "#Learns the parameter rho of the RCM model given a set of training data\n",
    "#training_data: The set of training data in the format of list of user action \n",
    "#return: the learned parameter rho\n",
    "def learns_param_RCM(training_data):\n",
    "    numb_doc = 0\n",
    "    num_click=0\n",
    "    for data in training_data:\n",
    "        #Only if the action is a query, we add to the total number of doc the number of doc returned by the query\n",
    "        if(\"Q\" in data[2]):\n",
    "            for j in range(5,len(data)):\n",
    "                numb_doc+=1\n",
    "        #If the action is a click, we add one to the total number of click \n",
    "        if(\"C\" in data[2]):\n",
    "            num_click+=1\n",
    "    return float(num_click)/numb_doc\n",
    "\n",
    "#Predicts the click probability given a ranked list of relevance labels\n",
    "#training_data: The set of training data in the format of list of user action \n",
    "#relevance_labels: A list of relevance labels (as integer) for the collection\n",
    "#return: A list of click probabilty (one probabilty per document)\n",
    "def predict_click_proba_RCM(training_data,relevance_labels) :\n",
    "    rho = learns_param_RCM(training_data)\n",
    "    #since the probability is rho for every document, \n",
    "    #we return a list of rho that has the length of the relevance labels list\n",
    "    return [rho]*len(relevance_labels)\n",
    "\n",
    "#Decides - stochastically -whether a document is clicked based on these probabilities\n",
    "#training_data: The set of training data in the format of list of user action \n",
    "#relevance_labels: A list of relevance labels (as integer) for the collection\n",
    "#document: The index of the document \n",
    "#return: A list of click probabilty (one probabilty per document)\n",
    "def is_doc_clicked_RCM(training_data,relevance_labels,document):\n",
    "    proba_distrib = predict_click_proba_RCM(training_data,relevance_labels)\n",
    "    return (bernoulli.rvs(proba_distrib[document])==1)\n",
    "\n",
    "def generate_click_RCM(training_data, interleaved_list):\n",
    "    click = []\n",
    "    proba_distrib = predict_click_proba_RCM(training_data,relevance_labels)\n",
    "    for i, elem in enumerate(proba_distrib):\n",
    "        click.append(bernoulli.rvs(elem))\n",
    "    return click\n",
    "\n",
    "#Simple Dynamic Bayesian Network (SDBN)\n",
    "\n",
    "#Add one to a dictionnary entry and create it if it doesn't exists\n",
    "#key: The key for which we want to add one in the dictionnary\n",
    "#dictionnary: \n",
    "#return: None\n",
    "def add1(key,dictionnary):\n",
    "    if key not in dictionnary:\n",
    "        dictionnary[key]=1\n",
    "    else:\n",
    "        dictionnary[key]+=1\n",
    "\n",
    "#Learns the parameter attractiveness and satisfaction of the SBDN model given a set of training data\n",
    "#training_data: The set of training data in the format of list of user action \n",
    "#return: the learned parameters attractiveness and satisfaction\n",
    "def learns_param_SBDN(training_data):\n",
    "    #the number of times document u is presented as response for the query q : used to calculate attrativness\n",
    "    doc_presented_q_dict= {}\n",
    "    #the number of times document u is clicked for the query q: used to calculate attrativness and satisfaction\n",
    "    doc_clicked_q_dict= {}\n",
    "    #the number of times document u is the last clicked for the query q: used to calculate satisfaction\n",
    "    doc_last_clicked_q_dict= {}\n",
    "    \n",
    "    global_index = 0 #index used to browser training_data\n",
    "    for session in range(int(training_data[len(training_data)-1][0])):\n",
    "        #index used to browser all the documents in a session\n",
    "        i=0 \n",
    "        end_of_session=False\n",
    "        #memorise the last query id\n",
    "        last_query =-1\n",
    "        #loop to browser the current session\n",
    "        while(not end_of_session):\n",
    "            #the session stop if the next index is not equal to the current session index or we reach the end of the list\n",
    "            if not (global_index+i+1)>= len(training_data):\n",
    "                end_of_session=(not int(training_data[global_index+i+1][0]) == session)\n",
    "            else:\n",
    "                end_of_session=True\n",
    "                \n",
    "            #memorize the current element for efficency reasons\n",
    "            element = training_data[global_index+i]\n",
    "            #memorize the last query\n",
    "            if(element[2]=='Q'):\n",
    "                last_query=int(element[3])\n",
    "                #memorize that a document appears for a query\n",
    "                for j in range(5,len(element)):\n",
    "                    key= (int(element[j]),last_query)\n",
    "                    add1(key,doc_presented_q_dict)\n",
    "            #memorize the click\n",
    "            if(element[2]=='C'):\n",
    "                key= (int(element[3]),last_query)\n",
    "                #memorize in the click dictionnary \n",
    "                #REMOVE\n",
    "                add1(key,doc_clicked_q_dict)\n",
    "                #If this is the last click of a session: memorize in the last click dictionnary \n",
    "                #or that the next element is a query \n",
    "                if end_of_session or (training_data[global_index+i+1][2]=='Q'):\n",
    "                    add1(key,doc_last_clicked_q_dict)\n",
    " \n",
    "            #increment the index used to browser the session \n",
    "            i+=1\n",
    "        #after the end of a session, skip to the next session\n",
    "        global_index+=i\n",
    "    \n",
    "    attract= {}\n",
    "    satisf = {}\n",
    "    #calculate attractivness which is #u is clicked for query q / #u appears with q \n",
    "    for key in doc_presented_q_dict:\n",
    "        if key in doc_clicked_q_dict:\n",
    "            attract[key]= float(doc_clicked_q_dict[key]) /  doc_presented_q_dict[key]\n",
    "        else: \n",
    "            attract[key]=0\n",
    "    #calculate satisfaction which is #u is last clicked document for query q / #u is clicked for query q \n",
    "    for key in doc_clicked_q_dict:\n",
    "        if key in doc_last_clicked_q_dict:\n",
    "            satisf[key]= float(doc_last_clicked_q_dict[key]) /  doc_clicked_q_dict[key]\n",
    "        else: \n",
    "            satisf[key]=0\n",
    "    return attract,satisf\n",
    "\n",
    "#Predicts the click probability given a ranked list of relevance labels\n",
    "#training_data: The set of training data in the format of list of user action \n",
    "#relevance_labels: A list of relevance labels (as integer) for the collection\n",
    "#doc_ids: The list of ids(with respect to the ids of the training data)  of the documents from the\n",
    "         #referance list \n",
    "#query: The query id (with respect to the ids of the training data).\n",
    "#return: A list of click probabilty (one probabilty per document)\n",
    "def predict_click_proba_SBDN(training_data,relevance_labels,doc_ids,query) :\n",
    "    attract,satisf= learns_param_SBDN(training_data)\n",
    "    proba= [0]* len(relevance_labels)\n",
    "    exam_proba=[0]*len(relevance_labels)\n",
    "    \n",
    "    for i in len(doc_ids):\n",
    "        #try to get the attractiveness for the document i and the query \n",
    "        alpha_i_q = 0\n",
    "        key = (doc_ids[i],query)\n",
    "        if key in attract:\n",
    "            alpha_i_q = attract[key]\n",
    "        \n",
    "        #try to get the satisfation for the document i and the query \n",
    "        sigma_i_q = 0 \n",
    "        if key in satisf:\n",
    "            sigma_i_q = satisf[key]\n",
    "            \n",
    "        #Check the text below to get how the probabilities and the the examination probability are updated\n",
    "        if(i==0):\n",
    "            proba[i]=alpha_i_q\n",
    "            exam_proba[i]=1\n",
    "        else :\n",
    "            proba[i]=alpha_i_q * exam_proba[i-1]\n",
    "            #gamma=1\n",
    "            exam_proba[i]=exam_proba[i-1] * ( (1-sigma_i_q)*alpha_i_q + (1-alpha_i_q) )\n",
    "    return proba\n",
    "\n",
    "#Decides - stochastically -whether a document is clicked based on these probabilities\n",
    "#training_data: The set of training data in the format of list of user action \n",
    "#relevance_labels: A list of relevance labels (as integer) for the collection\n",
    "#doc_ids: The list of ids(with respect to the ids of the training data)  of the documents from the\n",
    "         #referance list \n",
    "#query: The query id (with respect to the ids of the training data).\n",
    "#document: The index of the document \n",
    "#return: A list of click probabilty (one probabilty per document)\n",
    "def is_doc_clicked_SBDN(training_data,relevance_labels,doc_ids,query,document):\n",
    "    proba_distrib= predict_click_proba_SBDN(training_data,relevance_labels,doc_ids,query)\n",
    "    return (bernoulli.rvs(proba_distrib[document])==1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Rho: ', 0.13445559411047547)\n",
      "('Key', (29, 384), 'Expected (attract=', 0, 'satis=', 0, ') real: ', (0, 0))\n",
      "('Key', (89, 384), 'Expected (attract=', 0.2222222222222222, 'satis=', 1.0, ') real: ', (0.2222222222222222, 1.0))\n",
      "('Key', (293, 384), 'Expected (attract=', 0, 'satis=', 0, ') real: ', (0, 0))\n",
      "('Key', (394, 384), 'Expected (attract=', 0.75, 'satis=', 0, ') real: ', (0.75, 0))\n",
      "('Key', (742, 384), 'Expected (attract=', 0, 'satis=', 0, ') real: ', (0, 0))\n",
      "('Key', (867, 384), 'Expected (attract=', 0.75, 'satis=', 0.6666666666666666, ') real: ', (0.75, 0.6666666666666666))\n",
      "('Key', (1167, 384), 'Expected (attract=', 0.3076923076923077, 'satis=', 0.25, ') real: ', (0.3076923076923077, 0.25))\n",
      "('Key', (1653, 384), 'Expected (attract=', 0.2222222222222222, 'satis=', 0.5, ') real: ', (0.2222222222222222, 0.5))\n",
      "('Key', (3474, 384), 'Expected (attract=', 0.46153846153846156, 'satis=', 0.3333333333333333, ') real: ', (0.46153846153846156, 0.3333333333333333))\n",
      "('Key', (3494, 384), 'Expected (attract=', 0, 'satis=', 0, ') real: ', (0, 0))\n",
      "('Key', (3501, 384), 'Expected (attract=', 0.2222222222222222, 'satis=', 0.5, ') real: ', (0.2222222222222222, 0.5))\n",
      "('Key', (3558, 384), 'Expected (attract=', 0.07692307692307693, 'satis=', 0, ') real: ', (0.07692307692307693, 0))\n",
      "('Key', (8293, 384), 'Expected (attract=', 0.25, 'satis=', 0, ') real: ', (0.25, 0))\n",
      "('Key', (9217, 384), 'Expected (attract=', 0, 'satis=', 0, ') real: ', (0, 0))\n",
      "('Key', (15393, 384), 'Expected (attract=', 0.75, 'satis=', 0, ') real: ', (0.75, 0))\n",
      "('Key', (67395, 384), 'Expected (attract=', 0.25, 'satis=', 1.0, ') real: ', (0.25, 1.0))\n",
      "('Key', (67409, 384), 'Expected (attract=', 0.25, 'satis=', 0, ') real: ', (0.25, 0))\n"
     ]
    }
   ],
   "source": [
    "#TESTING \n",
    "\n",
    "#open file and preprocess data\n",
    "training_data=[]\n",
    "relevance_labels= [2, 1, 2, 1, 1, 0, 2, 1, 2, 2]\n",
    "document= 3\n",
    "\n",
    "f= open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "for line in f:\n",
    "    #The format is a list of user action \n",
    "    training_data.append(line.split(\"\\n\")[0].split('\\t'))\n",
    "f.close()\n",
    "print(\"Rho: \", learns_param_RCM(training_data))\n",
    "attract, satisf= learns_param_SBDN(training_data)\n",
    "#For testing SBDN, let's look at all the pairs of click/query for query 384 from the YandexRelPredChallenge.txt: \n",
    "keys = [(29,384),(89,384),(293,384),(394,384),(742,384),(867,384),(1167,384),(1653,384),(3474,384)\n",
    "        ,(3494,384),(3501,384),(3558,384),(8293,384),(9217,384),(15393,384),(67395,384),(67409,384)]\n",
    "\n",
    "#The following values have been determined by counting values from the YandexRelPredChallenge.txt. Those\n",
    "#values are summarized in the Analysis part below\n",
    "exp_attractiveness=[0,2.0/9.0,0,3.0/4.0,0,3.0/4.0,4.0/13.0,2.0/9.0,6.0/13.0,0,2.0/9.0,1.0/13.0,1.0/4.0,0,\n",
    "                    3.0/4.0,1.0/4.0,1.0/4.0]\n",
    "exp_satisfaction=[0,1.0,0,0,0,2.0/3.0,1.0/4.0,1.0/2.0,2.0/6.0,0,1.0/2.0,0,0,0,0,1.0,0]\n",
    "#Display the results \n",
    "for i in range(len(keys)):\n",
    "    key=keys[i]\n",
    "    att= 0\n",
    "    if key in attract:\n",
    "        att=attract[key]\n",
    "    sat= 0\n",
    "    if key in satisf:\n",
    "        sat= satisf[key]\n",
    "    print(\"Key\",key,\"Expected (attract=\",exp_attractiveness[i],\"satis=\",exp_satisfaction[i],\") real: \",\n",
    "          (att,sat))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanations\n",
    "\n",
    "For the RCM the learn parameter is calculated using the MLE. For this model, the probabilty of a click on a document is a constant always equal to the learned parameter $\\rho$. Finally, we used a Bernouilly process to stochastically decide if a document is clicked or not. \n",
    "\n",
    "For the SCDN, the parameters are also estimated using MLE: <br>\n",
    "$\\alpha_{uq}=\\frac{ number\\ of\\ document\\ u\\ clicked\\ for\\ query\\ u}{ number\\ of\\ document\\ u\\ appears\\ with\\ query\\ q} $ \n",
    "\n",
    "$\\sigma_{uq}=\\frac{ number\\ of\\ document\\ u\\ that\\ where\\ clicked\\ last\\ for\\ query\\ u}{ number\\ of\\ document\\ u\\ clicked\\ for\\ query\\ u} $. In our code we assume that a user can only click once on a document per session for a given query\n",
    "\n",
    "The formula then used to calculate the probability is: <br>\n",
    "$P(C_u=1) = \\alpha_{uq} \\epsilon_{r_u} $ <br>\n",
    "$\\epsilon_{r+1}= \\epsilon_{r}\\gamma  [(1-\\sigma_{uq})\\alpha_{uq} +(1-\\alpha_{uq})] $ with $\\epsilon$ the estimation probability\n",
    "\n",
    "Finally we also used a Bernouilly process to stochastically decide if a document is clicked or not. \n",
    "\n",
    "We ran into problems figuring out how to use the conditionnal click probabilty to build the click probability distribution since it requires to know if the previous documents where clicked or not. Since we are building a distribution, we don't know yet if the previous document will be clicked or not. We could also try to update the distribution every time the document is clicked but it then seems useless to have calculated the probability distribution since most of the values will be updated before being used. The formula to use to update the conditionnal probabilty is:<br>\n",
    "$(P(C_u=1|C_{<r_u})= \\alpha_{uq} \\epsilon_{r_u}$\n",
    "\n",
    "$\\epsilon_{r+1}= c_r^{(s)}\\gamma (1-\\sigma_{u_r q}) + (1-c_r^{(s)}) \\frac{(1-\\alpha_{u_r q}) \\epsilon_r \\gamma}{1-\\alpha_{u_r q}\\epsilon_r}  $\n",
    "\n",
    "For the attraction, we decided to calculate it via MLE in order to make those functions generic and accurate. \n",
    "##### Analysis\n",
    "The parameter rho for RCM has a value of 0.13 after evaluation thanks to the YandexRelPredChallenge.txt file. This value makes sense since the value in the litterature is around 0.12.\n",
    "\n",
    "For the SCDN parameters (attractiveness and satisfaction), as it is hard to see if those values make sense with fictive data, we decided to calculate them ourselves and then to compare with the computed values. We focused on the query 384 since it gave a decent number of examples with allows us to show that our code works well. Bellow are summarized the results we computed ourselves: \n",
    "\n",
    " Query 384 (number:13)\n",
    " Url list : 3474\t1653\t3501\t89\t1167\t3494\t3558\t742\t293\t29 (x9) <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;9217\t1167\t15393\t394\t867\t3474\t8293\t67409\t3558\t67395 (x4)\n",
    "* doc id: 29: appear:9, clicked:0 , last:0\n",
    "* doc id: 89: appear:9, clicked:2 , last:2\n",
    "* doc id: 293: appear:9, clicked:0 , last:0\n",
    "* doc id: 394: appear:4, clicked:3 , last:0\n",
    "* doc id: 742: appear:9, clicked:0 , last:0\n",
    "* doc id: 867: appear:4, clicked:3 , last:2\n",
    "* doc id: 1167: appear:13, clicked:4 , last:1 \n",
    "* doc id: 1653: appear:9, clicked:2 , last:1\n",
    "* doc id: 3474: appear:13, clicked:6 , last:2\n",
    "* doc id: 3494: appear:9, clicked:0 , last:0\n",
    "* doc id: 3501: appear:9, clicked:2 , last:1\n",
    "* doc id: 3558: appear:13, clicked:1 , last:0\n",
    "* doc id: 8293: appear:4, clicked:1 , last:0\t\n",
    "* doc id: 9217: appear:4, clicked:0 , last:0\n",
    "* doc id: 15393: appear:4, clicked:3 , last:0\n",
    "* doc id: 67395: appear:4, clicked:1 , last:1\n",
    "* doc id: 67409: appear:4, clicked:1 , last:0\t\n",
    " \n",
    "We finally found that the results we estimated were the same as the results we calculated with our code.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Simulate Interleaving Experiment ( 10 points)\n",
    "\n",
    "Having implemented the click models, it is time to run the simulated experiment.<br>\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and\n",
    "measure the proportion p of wins for E.<br>\n",
    "\n",
    "(Note 7: Some of the models above include an attractiveness parameter $\\alpha_{uq}$ . Use the relevance label to\n",
    "assign this parameter by setting $\\alpha_{uq}$ for a document u in the ranked list accordingly.)\n",
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For our case, we have to redined the attractivness and thus the previous functions of SBDN:\n",
    "#query value is a random number generated by generate_query\n",
    "#doc_ids ...                   generated by generate_doc_ids\n",
    "def specific_learns_param_SBDN(training_data,relevance_labels,query,doc_ids):\n",
    "    attract,satisf= learns_param_SBDN(training_data)\n",
    "    #the previous attractivness is incorrect for our case: we have to calculate it using relevance labels: \n",
    "    #thanks to formula p47 of \"Click Models for Web Search\",for a query q \n",
    "    \n",
    "    #relevance(u) = satisf(u) * attract(u) => attract(u) = relevance(u) / satisf(u)\n",
    "    relev_attract=[0]*len(relevance_labels)\n",
    "    for i in range(len(relevance_labels)):\n",
    "        satisf_u = 0\n",
    "        key = (doc_ids[i],query)\n",
    "        if key in satisf:\n",
    "            satisf_u = satisf[key]\n",
    "        if satisf_u != 0:\n",
    "            relev_attract[i]= relevance_labels[i]/satisf_u\n",
    "        else:\n",
    "            relev_attract[i] = 0\n",
    "    return relev_attract,satisf\n",
    "\n",
    "def generate_query(training_data):\n",
    "    query= set()\n",
    "    for data in training_data:\n",
    "        if data[2]==\"Q\":\n",
    "            query.add(int(data[3]))\n",
    "    return rand.sample(query,1)\n",
    "    \n",
    "def generate_doc_ids(training_data, number_ids, number_ids_different):\n",
    "    doc_id_set = set()\n",
    "    res=[]\n",
    "    for data in training_data:\n",
    "        if data[2]==\"Q\":\n",
    "            for j in range(5,len(data)):\n",
    "                doc_id_set.add(int(data[j]))\n",
    "   \n",
    "    for i in range(number_ids):\n",
    "        doc_id = rand.sample(doc_id_set,1)[0]\n",
    "        valid_id = (doc_id not in number_ids_different) and (doc_id not in res)\n",
    "        while (not valid_id):\n",
    "            doc_id = rand.sample(doc_id_set,1)[0]\n",
    "            valid_id = (doc_id not in number_ids_different) and (doc_id not in res)\n",
    "            valid_id= valid_id \n",
    "        res.append(doc_id)\n",
    "    return res \n",
    "        \n",
    "#query value is a random number generated by generate_query\n",
    "#doc_ids ...                   generated by generate_doc_ids\n",
    "def specific_predict_click_proba_SBDN(training_data,relevance_labels, query, doc_ids):\n",
    "    attract,satisf= specific_learns_param_SBDN(training_data,relevance_labels, query, doc_ids)\n",
    "    \n",
    "    proba= [0]* len(relevance_labels)\n",
    "    exam_proba=[0]*len(relevance_labels)\n",
    "    \n",
    "    for i in range(len(relevance_labels)):\n",
    "        alpha_i_q = attract[i]\n",
    "            \n",
    "        sigma_i_q = 0 \n",
    "        key = (doc_ids[i],query)\n",
    "        if key in satisf:\n",
    "            sigma_i_q = satisf[key]\n",
    "        \n",
    "        if(i==0):\n",
    "            proba[i]=alpha_i_q\n",
    "            exam_proba[i]=1\n",
    "        else :\n",
    "            proba[i]=alpha_i_q * exam_proba[i-1]\n",
    "            #gamma=1\n",
    "            exam_proba[i]=exam_proba[i-1] * ( (1-sigma_i_q)*alpha_i_q + (1-alpha_i_q) )\n",
    "    return proba\n",
    "#query value is a random number generated by generate_query\n",
    "#doc_ids ...                   generated by generate_doc_ids\n",
    "def specific_is_doc_clicked_SBDN(training_data,relevance_labels,doc_ids,query,document):\n",
    "    proba_distrib= specific_predict_click_proba_SBDN(training_data,relevance_labels,query,doc_ids)\n",
    "    return (bernoulli.rvs(proba_distrib[document])==1)\n",
    "\n",
    "def generate_click_SDBN(training_data, interleaved_list, query, doc_ids):\n",
    "    click = []\n",
    "    proba_distrib = specific_predict_click_proba_SBDN(training_data,relevance_labels, query, doc_ids)\n",
    "    for i, elem in enumerate(proba_distrib):\n",
    "        click.append(bernoulli.rvs(elem))\n",
    "    return click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pairs = rand.sample(pairs, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.515625\n",
      "0.64\n"
     ]
    }
   ],
   "source": [
    "# RCM simulation\n",
    "\n",
    "E_wins_proportion_td = 0\n",
    "E_wins_proportion_prob = 0\n",
    "tie_td = 0\n",
    "tie_prob = 0\n",
    "for i, elem in enumerate(test_pairs):\n",
    "    team_draft_res = team_draft_interleave(elem, False)\n",
    "    prob_res = probabilistic_interleave(elem)\n",
    "    clicks_td_RCM = generate_click_RCM(training_data, team_draft_res[1])\n",
    "    clicks_prob_RCM = generate_click_RCM(training_data, prob_res[1])\n",
    "    if(who_wins(team_draft_res[0], clicks_td_RCM) == \"E win\"):\n",
    "        E_wins_proportion_td += 1\n",
    "    elif(who_wins(team_draft_res[0], clicks_td_RCM) == \"tie\"):\n",
    "        tie_td += 1\n",
    "    if(who_wins(prob_res[0], clicks_prob_RCM) == \"E win\"):\n",
    "        E_wins_proportion_prob += 1\n",
    "    elif(who_wins(prob_res[0], clicks_prob_RCM) == \"tie\"):\n",
    "        tie_prob += 1\n",
    "\n",
    "print float(E_wins_proportion_td) / (100.0 - tie_td)\n",
    "print float(E_wins_proportion_prob) / (100.0 - tie_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_test = 384\n",
    "doc_ids_test = [29,89,293,394,742,867,1167,1653,3474,3494]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44\n",
      "0.54\n"
     ]
    }
   ],
   "source": [
    "# SDBN Simulation\n",
    "# Assumption, E and P always return different doc_ids, simulate one query at a time\n",
    "# Havent use generate doc ids and generate query function\n",
    "# Relevance labels is generated from interleaved list\n",
    "\n",
    "E_wins_proportion_td = 0\n",
    "E_wins_proportion_prob = 0\n",
    "tie_td = 0\n",
    "tie_prob = 0\n",
    "for i, elem in enumerate(test_pairs):\n",
    "    team_draft_res = team_draft_interleave(elem, False)\n",
    "    prob_res = probabilistic_interleave(elem)\n",
    "    clicks_td_SDBN = generate_click_SDBN(training_data, team_draft_res[1], query_test, doc_ids_test)\n",
    "    clicks_prob_SDBN = generate_click_SDBN(training_data, prob_res[1], query_test, doc_ids_test)\n",
    "    if(who_wins(team_draft_res[0], clicks_td_SDBN) == \"E win\"):\n",
    "        E_wins_proportion_td += 1\n",
    "    elif(who_wins(team_draft_res[0], clicks_td_SDBN) == \"tie\"):\n",
    "        tie_td += 1\n",
    "    if(who_wins(prob_res[0], clicks_prob_SDBN) == \"E win\"):\n",
    "        E_wins_proportion_prob += 1\n",
    "    elif(who_wins(prob_res[0], clicks_prob_SDBN) == \"tie\"):\n",
    "        tie_prob += 1\n",
    "        \n",
    "print float(E_wins_proportion_td) / (100.0 - tie_td)\n",
    "print float(E_wins_proportion_prob) / (100.0 - tie_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To do :\n",
    "- More verbality\n",
    "- Explanation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
