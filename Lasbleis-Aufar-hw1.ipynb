{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 : Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the first step you will generate pair s of rankings of relevance, for the production P and experimental E,\n",
    "respectively, for a hypothetical query q . Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all\n",
    "possible P and E ranking pairs of length 5, for which E outperforms P.\n",
    "\n",
    "Example:\n",
    "\n",
    "P: {N N N N N}\n",
    "\n",
    "E: {N N N N R}\n",
    "\n",
    "…\n",
    "\n",
    "P: {HR HR HR HR R}\n",
    "\n",
    "E: {HR HR HR HR HR}\n",
    "\n",
    "(Note 1: If you do not have enough computational power, sample 1000 pair uniformly at random to show\n",
    "your work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert int to relevance\n",
    "def to_string(_list): \n",
    "    L=[]\n",
    "    for i in _list:\n",
    "        if i==0:\n",
    "            L.append(\"N \")\n",
    "        elif i==1:\n",
    "            L.append(\"R \")\n",
    "        elif i==2:\n",
    "            L.append(\"HR\")\n",
    "        else: \n",
    "            L.append(\"? \")\n",
    "    return L\n",
    " \n",
    "def outperforms(E,P):\n",
    "    for i in range(len(E)):\n",
    "        if E[i]<P[i]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs = [] \n",
    "for p_possibility in range(243): #3^5 = 243 \n",
    "    P=[None]*5\n",
    "    for p_pos in range(5):\n",
    "        P[p_pos]= p_possibility // (np.power(3,p_pos)) %3\n",
    "    for e_possibility in range(243): #3^5 = 243 \n",
    "        E=[None]*5\n",
    "        for e_pos in range(5):\n",
    "            E[e_pos]= e_possibility // (np.power(3,e_pos)) %3\n",
    "        #if(outperforms(E,P)):\n",
    "            #pairs.append((P,E))\n",
    "        pairs.append((P,E))\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59049\n",
      "P:  ['R ', 'R ', 'HR', 'N ', 'N '] E:  ['R ', 'HR', 'R ', 'R ', 'R ']\n"
     ]
    }
   ],
   "source": [
    "print(len(pairs))\n",
    "r = rand.randint(0,7775)\n",
    "print(\"P: \" , to_string(pairs[r][0]), \"E: \",to_string(pairs[r][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Step 2 : Implement Evaluation Measures (15 points)\n",
    "\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above.\n",
    "\n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant\n",
    "documents in the entire collection – pay extra attention on how to find this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: ================================================================\n",
    "\n",
    "========================================================================\n",
    "\n",
    "===========================================================================\n",
    "\n",
    "add if security if the number of relevant is null ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: choose 1 out of 3\n",
    "#BINARY \n",
    "#Precision at rank r \n",
    "def Binary_measure_precision(testSet, r):\n",
    "    num_test_rel = 0 \n",
    "    for i in range(len(testSet)):\n",
    "        if testSet[i]>0:\n",
    "            if i<r:\n",
    "                num_test_rel+=1\n",
    "    return float(num_test_rel) / r\n",
    "\n",
    "#Recall at rank k \n",
    "def Binary_measure_recal(testSet, r):\n",
    "    num_test_rel = 0 \n",
    "    total_num_rel=0\n",
    "    for i in range(len(testSet)):\n",
    "        if testSet[i]>0:\n",
    "            if i<r:\n",
    "                num_test_rel+=1\n",
    "            total_num_rel+=1\n",
    "    return float(num_test_rel) / total_num_rel\n",
    "\n",
    "#Average precision at rank k \n",
    "def Binary_measure_avg_precision(testSet, r):\n",
    "    num_rel=0\n",
    "    sum_precision = 0 \n",
    "    total_num_rel=0\n",
    "    temp=\"(\"\n",
    "    for i in range(len(testSet)):\n",
    "        if testSet[i]>0:\n",
    "            if i<r:\n",
    "                num_rel+=1\n",
    "                temp+=\" \"+str(num_rel)+\"/\"+str(i+1)+\"+\"\n",
    "                sum_precision+=float(num_rel)/(i+1)\n",
    "            total_num_rel+=1\n",
    "    temp=temp[:len(temp)-1]+\")\"\n",
    "    #print(temp,\"/\",total_num_rel)\n",
    "    return float(sum_precision) / total_num_rel\n",
    "\n",
    "#MULTI GRADED \n",
    "#Discount cumulative gain DCG at rank r\n",
    "def DCG(testSet, r):\n",
    "    res=0\n",
    "    for k in range(1,r+1):#r starts at 1\n",
    "        rel_r = testSet[k-1]\n",
    "        res+=float(np.power(2,rel_r)-1)/(np.log2(1+k))\n",
    "    return res\n",
    "\n",
    "#Normalized discount cumulative gain nDCG at rank r\n",
    "def nDCG(testSet, r):\n",
    "    orderedSet= list(testSet)\n",
    "    orderedSet.sort(reverse=True)\n",
    "    res=0\n",
    "    for k in range(1,r+1):\n",
    "        res+=DCG(testSet, k) / DCG(orderedSet, k)\n",
    "    return res/k\n",
    "#Rank Biased Precision w ith persistence parameter 𝜃 =0.8\n",
    "def RBP (testSet,theta=0.8):\n",
    "    res=0\n",
    "    for k in range(1,len(testSet)+1):\n",
    "        rel_k=testSet[k-1]\n",
    "        res+= rel_k * np.power(theta,k-1) * (1.0-theta)\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.5\n",
      "Recal 1.0\n",
      "Average precision 0.6155128205128204\n",
      "DCG 3.18738348176\n",
      "nDCG 0.790962232212\n",
      "RBP 0.897885306542\n"
     ]
    }
   ],
   "source": [
    "test= [2,0,2,1,0,1,0,0,0,1,0,1,1,0,1,0,0,1,0,1]#length 20, from example p 25 in Lecture 1b \n",
    "r=20\n",
    "print('Precision',Binary_measure_precision(test,r))\n",
    "print('Recal',Binary_measure_recal(test,r))\n",
    "print('Average precision', Binary_measure_avg_precision(test,r))\n",
    "print('DCG', DCG(test,r))\n",
    "print('nDCG', nDCG(test,r))\n",
    "print('RBP', RBP(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#precision , DCG, RBP \n",
    "outperf_pairs=[]\n",
    "d_measure_precision=[]\n",
    "d_measure_DCG=[]\n",
    "d_measure_RBP=[]\n",
    "\n",
    "rank = 4\n",
    "for i in range(len(pairs)):#pairs made of (P,E)\n",
    "    if(outperforms(pairs[i][1],pairs[i][0])):\n",
    "        outperf_pairs.append(pairs[i])\n",
    "        d_measure_precision.append(Binary_measure_precision(pairs[i][1],rank)-Binary_measure_precision(pairs[i][0],rank))\n",
    "        d_measure_DCG.append(DCG(pairs[i][1],rank)-DCG(pairs[i][0],rank))\n",
    "        d_measure_RBP.append(RBP(pairs[i][1])-RBP(pairs[i][0]))\n",
    "\n",
    "#for i in range(len(outperf_pairs)):\n",
    "    #print(\"P:\",to_string(outperf_pairs[i][0]),\"E:\",to_string(outperf_pairs[i][1]))\n",
    "    #print(d_measure_precision[i])\n",
    "    #print(d_measure_DCG[i])\n",
    "    #print(d_measure_RBP[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to specify what outperforms means: \n",
    "-E[rank]>P[rank] for all rank (I'll say either this one)\n",
    "-E[rank]>P[rank] for a rank and for a specific measure\n",
    "-E[rank]>P[rank] for all rank and for a specific measure\n",
    "-E[rank]>P[rank] for all rank and for all specific measure (or this one)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
