{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 Code and Report\n",
    "AUFAR Rezka and LASBLEIS Alexandre "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the first step you will generate pair s of rankings of relevance, for the production P and experimental E,\n",
    "respectively, for a hypothetical query q . Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all\n",
    "possible P and E ranking pairs of length 5, for which E outperforms P.\n",
    "\n",
    "Example:<br>\n",
    "P: {N N N N N}<br>\n",
    "E: {N N N N R}<br>\n",
    "… <br>\n",
    "P: {HR HR HR HR R}<br>\n",
    "E: {HR HR HR HR HR}<br>\n",
    "\n",
    "(Note 1: If you do not have enough computational power, sample 1000 pair uniformly at random to show\n",
    "your work.)\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert int to relevance\n",
    "#_list : list of integer to be converted\n",
    "#return : list of string where every integer has been replaced by its relevance interpretation\n",
    "def to_string(_list): \n",
    "    L=[]\n",
    "    for i in _list:\n",
    "        if i==0:\n",
    "            L.append(\"N \")\n",
    "        elif i==1:\n",
    "            L.append(\"R \")\n",
    "        elif i==2:\n",
    "            L.append(\"HR\")\n",
    "        else: \n",
    "            L.append(\"? \")\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs = [] #variable used to store all pairs \n",
    "Ps = []\n",
    "Es = []\n",
    "\n",
    "#Create all combinaison for P, there are 3^5 = 243 possibilities since P is of length 5 and each member have 3 possible values:\n",
    "# 0 (representing \"N\" ), 1 (representing \"R), 2 (representing \"HR)\n",
    "for p_possibility in range(243): \n",
    "    P=[None]*5\n",
    "    #create all five values for P \n",
    "    for p_pos in range(5):\n",
    "        P[p_pos]= p_possibility // (np.power(3,p_pos)) %3 #formula to create all combinaison\n",
    "    #we do the same for E    \n",
    "    for e_possibility in range(243): \n",
    "        E=[None]*5\n",
    "        for e_pos in range(5):\n",
    "            E[e_pos]= e_possibility // (np.power(3,e_pos)) %3 #formula to create all combinaison\n",
    "        pairs.append((P,E)) #adding the pair since every pair is valid\n",
    "        #Memorize all created Es but only once\n",
    "        if p_possibility==0:\n",
    "            Es.append(E)\n",
    "        #memorize all created Ps\n",
    "    Ps.append(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59049\n",
      "('P: ', ['N ', 'R ', 'R ', 'R ', 'R '], 'E: ', ['N ', 'N ', 'R ', 'N ', 'HR'])\n",
      "243\n",
      "[[0, 0, 0, 0, 0], [1, 0, 0, 0, 0], [2, 0, 0, 0, 0], [0, 1, 0, 0, 0], [1, 1, 0, 0, 0], [2, 1, 0, 0, 0], [0, 2, 0, 0, 0], [1, 2, 0, 0, 0], [2, 2, 0, 0, 0], [0, 0, 1, 0, 0], [1, 0, 1, 0, 0], [2, 0, 1, 0, 0], [0, 1, 1, 0, 0], [1, 1, 1, 0, 0], [2, 1, 1, 0, 0], [0, 2, 1, 0, 0], [1, 2, 1, 0, 0], [2, 2, 1, 0, 0], [0, 0, 2, 0, 0], [1, 0, 2, 0, 0], [2, 0, 2, 0, 0], [0, 1, 2, 0, 0], [1, 1, 2, 0, 0], [2, 1, 2, 0, 0], [0, 2, 2, 0, 0], [1, 2, 2, 0, 0], [2, 2, 2, 0, 0], [0, 0, 0, 1, 0], [1, 0, 0, 1, 0], [2, 0, 0, 1, 0], [0, 1, 0, 1, 0], [1, 1, 0, 1, 0], [2, 1, 0, 1, 0], [0, 2, 0, 1, 0], [1, 2, 0, 1, 0], [2, 2, 0, 1, 0], [0, 0, 1, 1, 0], [1, 0, 1, 1, 0], [2, 0, 1, 1, 0], [0, 1, 1, 1, 0], [1, 1, 1, 1, 0], [2, 1, 1, 1, 0], [0, 2, 1, 1, 0], [1, 2, 1, 1, 0], [2, 2, 1, 1, 0], [0, 0, 2, 1, 0], [1, 0, 2, 1, 0], [2, 0, 2, 1, 0], [0, 1, 2, 1, 0], [1, 1, 2, 1, 0], [2, 1, 2, 1, 0], [0, 2, 2, 1, 0], [1, 2, 2, 1, 0], [2, 2, 2, 1, 0], [0, 0, 0, 2, 0], [1, 0, 0, 2, 0], [2, 0, 0, 2, 0], [0, 1, 0, 2, 0], [1, 1, 0, 2, 0], [2, 1, 0, 2, 0], [0, 2, 0, 2, 0], [1, 2, 0, 2, 0], [2, 2, 0, 2, 0], [0, 0, 1, 2, 0], [1, 0, 1, 2, 0], [2, 0, 1, 2, 0], [0, 1, 1, 2, 0], [1, 1, 1, 2, 0], [2, 1, 1, 2, 0], [0, 2, 1, 2, 0], [1, 2, 1, 2, 0], [2, 2, 1, 2, 0], [0, 0, 2, 2, 0], [1, 0, 2, 2, 0], [2, 0, 2, 2, 0], [0, 1, 2, 2, 0], [1, 1, 2, 2, 0], [2, 1, 2, 2, 0], [0, 2, 2, 2, 0], [1, 2, 2, 2, 0], [2, 2, 2, 2, 0], [0, 0, 0, 0, 1], [1, 0, 0, 0, 1], [2, 0, 0, 0, 1], [0, 1, 0, 0, 1], [1, 1, 0, 0, 1], [2, 1, 0, 0, 1], [0, 2, 0, 0, 1], [1, 2, 0, 0, 1], [2, 2, 0, 0, 1], [0, 0, 1, 0, 1], [1, 0, 1, 0, 1], [2, 0, 1, 0, 1], [0, 1, 1, 0, 1], [1, 1, 1, 0, 1], [2, 1, 1, 0, 1], [0, 2, 1, 0, 1], [1, 2, 1, 0, 1], [2, 2, 1, 0, 1], [0, 0, 2, 0, 1], [1, 0, 2, 0, 1], [2, 0, 2, 0, 1], [0, 1, 2, 0, 1], [1, 1, 2, 0, 1], [2, 1, 2, 0, 1], [0, 2, 2, 0, 1], [1, 2, 2, 0, 1], [2, 2, 2, 0, 1], [0, 0, 0, 1, 1], [1, 0, 0, 1, 1], [2, 0, 0, 1, 1], [0, 1, 0, 1, 1], [1, 1, 0, 1, 1], [2, 1, 0, 1, 1], [0, 2, 0, 1, 1], [1, 2, 0, 1, 1], [2, 2, 0, 1, 1], [0, 0, 1, 1, 1], [1, 0, 1, 1, 1], [2, 0, 1, 1, 1], [0, 1, 1, 1, 1], [1, 1, 1, 1, 1], [2, 1, 1, 1, 1], [0, 2, 1, 1, 1], [1, 2, 1, 1, 1], [2, 2, 1, 1, 1], [0, 0, 2, 1, 1], [1, 0, 2, 1, 1], [2, 0, 2, 1, 1], [0, 1, 2, 1, 1], [1, 1, 2, 1, 1], [2, 1, 2, 1, 1], [0, 2, 2, 1, 1], [1, 2, 2, 1, 1], [2, 2, 2, 1, 1], [0, 0, 0, 2, 1], [1, 0, 0, 2, 1], [2, 0, 0, 2, 1], [0, 1, 0, 2, 1], [1, 1, 0, 2, 1], [2, 1, 0, 2, 1], [0, 2, 0, 2, 1], [1, 2, 0, 2, 1], [2, 2, 0, 2, 1], [0, 0, 1, 2, 1], [1, 0, 1, 2, 1], [2, 0, 1, 2, 1], [0, 1, 1, 2, 1], [1, 1, 1, 2, 1], [2, 1, 1, 2, 1], [0, 2, 1, 2, 1], [1, 2, 1, 2, 1], [2, 2, 1, 2, 1], [0, 0, 2, 2, 1], [1, 0, 2, 2, 1], [2, 0, 2, 2, 1], [0, 1, 2, 2, 1], [1, 1, 2, 2, 1], [2, 1, 2, 2, 1], [0, 2, 2, 2, 1], [1, 2, 2, 2, 1], [2, 2, 2, 2, 1], [0, 0, 0, 0, 2], [1, 0, 0, 0, 2], [2, 0, 0, 0, 2], [0, 1, 0, 0, 2], [1, 1, 0, 0, 2], [2, 1, 0, 0, 2], [0, 2, 0, 0, 2], [1, 2, 0, 0, 2], [2, 2, 0, 0, 2], [0, 0, 1, 0, 2], [1, 0, 1, 0, 2], [2, 0, 1, 0, 2], [0, 1, 1, 0, 2], [1, 1, 1, 0, 2], [2, 1, 1, 0, 2], [0, 2, 1, 0, 2], [1, 2, 1, 0, 2], [2, 2, 1, 0, 2], [0, 0, 2, 0, 2], [1, 0, 2, 0, 2], [2, 0, 2, 0, 2], [0, 1, 2, 0, 2], [1, 1, 2, 0, 2], [2, 1, 2, 0, 2], [0, 2, 2, 0, 2], [1, 2, 2, 0, 2], [2, 2, 2, 0, 2], [0, 0, 0, 1, 2], [1, 0, 0, 1, 2], [2, 0, 0, 1, 2], [0, 1, 0, 1, 2], [1, 1, 0, 1, 2], [2, 1, 0, 1, 2], [0, 2, 0, 1, 2], [1, 2, 0, 1, 2], [2, 2, 0, 1, 2], [0, 0, 1, 1, 2], [1, 0, 1, 1, 2], [2, 0, 1, 1, 2], [0, 1, 1, 1, 2], [1, 1, 1, 1, 2], [2, 1, 1, 1, 2], [0, 2, 1, 1, 2], [1, 2, 1, 1, 2], [2, 2, 1, 1, 2], [0, 0, 2, 1, 2], [1, 0, 2, 1, 2], [2, 0, 2, 1, 2], [0, 1, 2, 1, 2], [1, 1, 2, 1, 2], [2, 1, 2, 1, 2], [0, 2, 2, 1, 2], [1, 2, 2, 1, 2], [2, 2, 2, 1, 2], [0, 0, 0, 2, 2], [1, 0, 0, 2, 2], [2, 0, 0, 2, 2], [0, 1, 0, 2, 2], [1, 1, 0, 2, 2], [2, 1, 0, 2, 2], [0, 2, 0, 2, 2], [1, 2, 0, 2, 2], [2, 2, 0, 2, 2], [0, 0, 1, 2, 2], [1, 0, 1, 2, 2], [2, 0, 1, 2, 2], [0, 1, 1, 2, 2], [1, 1, 1, 2, 2], [2, 1, 1, 2, 2], [0, 2, 1, 2, 2], [1, 2, 1, 2, 2], [2, 2, 1, 2, 2], [0, 0, 2, 2, 2], [1, 0, 2, 2, 2], [2, 0, 2, 2, 2], [0, 1, 2, 2, 2], [1, 1, 2, 2, 2], [2, 1, 2, 2, 2], [0, 2, 2, 2, 2], [1, 2, 2, 2, 2], [2, 2, 2, 2, 2]]\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "print(len(pairs)) #= 59049 which is equal to the expected length = 3 ^10 = 59049\n",
    "#print(pairs) #print all pairs but this takes a lot of place\n",
    "\n",
    "#print a random pair to show that it makes sense \n",
    "r = rand.randint(0,59049-1)\n",
    "print(\"P: \" , to_string(pairs[r][0]), \"E: \",to_string(pairs[r][1]))\n",
    "\n",
    "#check if Es is as expected \n",
    "print(len(Es))#expect 243 = 3^5 \n",
    "print(Es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanations\n",
    "In order to contruct all possible pairs of P and E with a 3-graded relevance {N, R, HR}, we first encode those relevance as integers: <br>\n",
    "* 0 for N <br>\n",
    "* 1 for R <br>\n",
    "* 2 for HR <br>\n",
    "This encoding allows us to do easy comparison between elements.\n",
    "Then we created all pairs using this formula : $value = \\frac{possibility}{3^{pos}} [3]$ with $possibility \\in \\{0,...,243\\}$ and $ pos \\in \\{0,1,2,3,4\\}$.\n",
    "\n",
    "This give the following sequence depending on the position of the element in the list: <br>\n",
    "* 0: $\\{0,1,2,0,1,2,0,1,2,0,1,2...\\}$\n",
    "* 1: $\\{0,0,0,1,1,1,2,2,2,0,0,0...\\}$\n",
    "* 2: $\\{0,0,0,0,0,0,0,0,0,1,1,1,...\\}$\n",
    "* ... and so on\n",
    "\n",
    "This formula is then used for the production P and the experiment E. \n",
    "\n",
    "##### Analysis\n",
    "According to the formula we showed before, we should find all the possible combination. Futhermore, the number of pairs found is equal to the one we expected $3^{10}$ and the number of E and P are $3^{5}$ each which is also what is expected. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 2 : Implement Evaluation Measures (15 points)\n",
    "\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above.\n",
    "\n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant\n",
    "documents in the entire collection – pay extra attention on how to find this)\n",
    "\n",
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Remove for evaluation \n",
    "\n",
    "#Recall at rank k \n",
    "def Binary_measure_recal(test_set, r):\n",
    "    num_test_rel = 0 \n",
    "    total_num_rel=0\n",
    "    for i in range(len(test_set)):\n",
    "        if test_set[i]>0:\n",
    "            if i<r:\n",
    "                num_test_rel+=1\n",
    "            total_num_rel+=1\n",
    "    return float(num_test_rel) / total_num_rel\n",
    "\n",
    "#Average precision at rank k \n",
    "def Binary_measure_avg_precision(test_set, r):\n",
    "    num_rel=0\n",
    "    sum_precision = 0 \n",
    "    total_num_rel=0\n",
    "    temp=\"(\"\n",
    "    for i in range(len(test_set)):\n",
    "        if test_set[i]>0:\n",
    "            if i<r:\n",
    "                num_rel+=1\n",
    "                temp+=\" \"+str(num_rel)+\"/\"+str(i+1)+\"+\"\n",
    "                sum_precision+=float(num_rel)/(i+1)\n",
    "            total_num_rel+=1\n",
    "    temp=temp[:len(temp)-1]+\")\"\n",
    "    #print(temp,\"/\",total_num_rel)\n",
    "    return float(sum_precision) / total_num_rel\n",
    "#Normalized discount cumulative gain nDCG at rank r\n",
    "def nDCG(test_set, r):\n",
    "    orderedSet= list(test_set)\n",
    "    orderedSet.sort(reverse=True)\n",
    "    return DCG(test_set, r) / DCG(orderedSet, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BINARY \n",
    "#Precision at rank r \n",
    "#test_set: the collection to be tested \n",
    "#r: the rank for which we test the collection \n",
    "#return : the precision \n",
    "def Binary_measure_precision(test_set, r):\n",
    "    assert r <= len(test_set)\n",
    "    assert r!= 0\n",
    "    num_test_rel = 0 \n",
    "    for i in range(r):\n",
    "        #According to our notation, if test_set[i]>0, the document is relevant\n",
    "        if test_set[i]>0:\n",
    "            num_test_rel+=1\n",
    "    return float(num_test_rel) / r #precision is number of relevant doc/ number of doc considered\n",
    "\n",
    "#MULTI GRADED \n",
    "#Discount cumulative gain DCG at rank r\n",
    "#test_set: the collection to be tested \n",
    "#r: the rank for which we test the collection \n",
    "#return : the gain \n",
    "def DCG(test_set, r):\n",
    "    assert r <= len(test_set)\n",
    "    assert r!= 0\n",
    "    res=0\n",
    "    for k in range(1,r+1):#r is the rank so it starts at 1\n",
    "        rel_r = test_set[k-1]#index of element = rank -1 since rank starts at 1 and index at 0\n",
    "        res+=float(np.power(2,rel_r)-1)/(np.log2(1+k))\n",
    "    return res\n",
    "\n",
    "#Rank Biased Precision w ith persistence parameter 𝜃 =0.8\n",
    "#test_set: the collection to be tested \n",
    "#theta: the theta parameter of the RBP formula\n",
    "#return : the precision\n",
    "def RBP (test_set,theta=0.8):\n",
    "    res=0\n",
    "    for k in range(1,len(test_set)+1):\n",
    "        rel_k=test_set[k-1]\n",
    "        res+= rel_k * np.power(theta,k-1) * (1.0-theta)\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific ordering\n",
      "('Rank = ', 1, 'Precision', 1.0, 'DCG', 1.0, 'RBP', 1.06)\n",
      "('Rank = ', 2, 'Precision', 0.5, 'DCG', 1.0, 'RBP', 1.06)\n",
      "('Rank = ', 3, 'Precision', 0.667, 'DCG', 2.5, 'RBP', 1.06)\n",
      "('Rank = ', 4, 'Precision', 0.75, 'DCG', 15.851, 'RBP', 1.06)\n",
      "('Rank = ', 5, 'Precision', 0.6, 'DCG', 15.851, 'RBP', 1.06)\n",
      "('Rank = ', 6, 'Precision', 0.667, 'DCG', 16.207, 'RBP', 1.06)\n",
      "('Rank = ', 7, 'Precision', 0.571, 'DCG', 16.207, 'RBP', 1.06)\n",
      "('Rank = ', 8, 'Precision', 0.5, 'DCG', 16.207, 'RBP', 1.06)\n",
      "('Rank = ', 9, 'Precision', 0.444, 'DCG', 16.207, 'RBP', 1.06)\n",
      "('Rank = ', 10, 'Precision', 0.5, 'DCG', 16.496, 'RBP', 1.06)\n",
      "\n",
      "Perfect ordering\n",
      "('Rank = ', 1, 'Precision', 1.0, 'DCG', 31.0, 'RBP', 1.632)\n",
      "('Rank = ', 2, 'Precision', 1.0, 'DCG', 32.893, 'RBP', 1.632)\n",
      "('Rank = ', 3, 'Precision', 1.0, 'DCG', 33.393, 'RBP', 1.632)\n",
      "('Rank = ', 4, 'Precision', 1.0, 'DCG', 33.823, 'RBP', 1.632)\n",
      "('Rank = ', 5, 'Precision', 1.0, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 6, 'Precision', 0.833, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 7, 'Precision', 0.714, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 8, 'Precision', 0.625, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 9, 'Precision', 0.556, 'DCG', 34.21, 'RBP', 1.632)\n",
      "('Rank = ', 10, 'Precision', 0.5, 'DCG', 34.21, 'RBP', 1.632)\n",
      "\n",
      "Worst ordering\n",
      "('Rank = ', 1, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 2, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 3, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 4, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 5, 'Precision', 0.0, 'DCG', 0.0, 'RBP', 0.361)\n",
      "('Rank = ', 6, 'Precision', 0.167, 'DCG', 0.356, 'RBP', 0.361)\n",
      "('Rank = ', 7, 'Precision', 0.286, 'DCG', 0.69, 'RBP', 0.361)\n",
      "('Rank = ', 8, 'Precision', 0.375, 'DCG', 1.005, 'RBP', 0.361)\n",
      "('Rank = ', 9, 'Precision', 0.444, 'DCG', 1.908, 'RBP', 0.361)\n",
      "('Rank = ', 10, 'Precision', 0.5, 'DCG', 10.869, 'RBP', 0.361)\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "test=        [1,0,2,5,0,1,0,0,0,1]#length 10\n",
    "test_perfect=[5,2,1,1,1,0,0,0,0,0]\n",
    "test_worst=  [0,0,0,0,0,1,1,1,2,5]\n",
    "\n",
    "#The following results are rounded in order to make them readable\n",
    "#Specific ordering \n",
    "print(\"Specific ordering\")\n",
    "for r in range(1,len(test)+1):\n",
    "    print('Rank = ' , r,'Precision',round(Binary_measure_precision(test,r),3),'DCG', round(DCG(test,r),3),\n",
    "          'RBP', round(RBP(test),3))\n",
    "\n",
    "#Perfect ordering \n",
    "print(\"\\nPerfect ordering\")\n",
    "for r in range(1,len(test_perfect)+1):\n",
    "    print('Rank = ' , r,'Precision',round(Binary_measure_precision(test_perfect,r),3),'DCG', round(DCG(test_perfect,r),3),\n",
    "          'RBP', round(RBP(test_perfect),3))\n",
    "\n",
    "#Worst ordering \n",
    "print(\"\\nWorst ordering\")\n",
    "for r in range(1,len(test_worst)+1):\n",
    "    print('Rank = ' , r,'Precision',round(Binary_measure_precision(test_worst,r),3),'DCG', round(DCG(test_worst,r),3),\n",
    "          'RBP', round(RBP(test_worst),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanations\n",
    "For Precision and DCG, we need to ensure that rank k does not exceed the documents length, and k cannot be zero.<br>\n",
    "For DCG and RBP, we also decided that the value of the relevant would be equal to the value at that rank in the list. For example if an example is extremely relevant, he might get a value of 5 (as in the example) which leads to higher gain in DCG than a document which is only highly relevant (value of 2).<br>\n",
    "Otherwise our implementation follows the definition of the method. \n",
    "\n",
    "##### Analysis\n",
    "The result for Precision is highly dependant on the number of non-relevant in the cut-off collection. As k increases, the more non-relevant exists in the cut-off, the lower the precision will be. If we consider a rank equal to the length of the list, the precision is the same whatever the order is. Precision does not that into account how good the relevance of a document is, it only cares if a document is relevant or not. \n",
    "\n",
    "For DCG, the result depends on the degree of the relevance and the order of it. If the highest relevant score exists in the first position of the cut-off, the higher DCG will be. We showed that by displaying the DCG score with perfect ordering which is higher at any rank than the \"specific ordering\". Those value may be hard to use to compare different collections due to the fact that it is sensitive to the degree of relevance. A way to cope with that problem is to use its alternative version: the nDCG (normalized version). \n",
    "\n",
    "For RBP, we disregard rank and therefore we have the same result no matter the rank is. This measure pays great attention to the ordering as shown above: the best score is obtained with a perfect ordering whereas the worst one is obtained with the worst ordering. The model behind RBP considers that when you see a document, you have a probability to see the next document otherwise you stop. This model can be improved by taking into account how relevant is the document that we are looking at. Depending on this relevance, there is a probability to see the next document or to stop. This second model is called ERR model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Calculate the measure (5 points)\n",
    "\n",
    "For the three measures and all P and E ranking pairs constructed above calculate the difference: 𝛥measure\n",
    "= measure E -measure P . Consider only those pairs for which E outperforms P.\n",
    "\n",
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluate if E outperfoms P with respect to a specific measure. \n",
    "#methond_name: the measure used to test. It can be \"precision\", \"DCG\" or \"RBP\"\n",
    "#E: \n",
    "#P:\n",
    "#rank: \n",
    "#return : (outperforms, delta measure) where outperforms is True if E outperfors P with respect to the measure method for the\n",
    "#rank and delta measure is the difference between to measure of E and P for the measure method.\n",
    "def outperforms(methond_name,E,P,rank):\n",
    "    if methond_name==\"precision\":\n",
    "        delta = Binary_measure_precision(E,rank)-Binary_measure_precision(P,rank)\n",
    "        return (delta>0), delta \n",
    "    elif methond_name==\"DCG\":\n",
    "        delta=DCG(E,rank)-DCG(P,rank)\n",
    "        return (delta>0), delta \n",
    "    elif methond_name==\"RBP\":\n",
    "        delta=RBP(E)-RBP(P)\n",
    "        return (delta>0), delta \n",
    "\n",
    "#variable for precision measure \n",
    "outperf_pairs_prec=[] #memorize pairs for which E outperfoms P\n",
    "pairs_set_prec = [] \n",
    "d_measure_prec=[] #memorize \"measure(E)-measure(P)\" for which E outperfoms P\n",
    "\n",
    "#variable for DCG measure \n",
    "outperf_pairs_DCG=[] #memorize pairs for which E outperfoms P\n",
    "pairs_set_DCG = []\n",
    "d_measure_DCG=[] #memorize \"measure(E)-measure(P)\" for which E outperfoms P\n",
    "\n",
    "#variable for RBP measure\n",
    "outperf_pairs_RBP=[] #memorize pairs for which E outperfoms P\n",
    "pairs_set_RBP = []\n",
    "d_measure_RBP=[] #memorize \"measure(E)-measure(P)\" for which E outperfoms P\n",
    "\n",
    "rank = 4\n",
    "for i in range(len(pairs)):#pairs made of (P,E)\n",
    "    #test if E outperforms P with respect to a specific method\n",
    "    outperforms_prec,delta_prec= outperforms(\"precision\",pairs[i][1],pairs[i][0],rank)\n",
    "    outperforms_DCG,delta_DCG= outperforms(\"DCG\",pairs[i][1],pairs[i][0],rank)\n",
    "    outperforms_RBP,delta_RBP= outperforms(\"RBP\",pairs[i][1],pairs[i][0],rank)\n",
    "\n",
    "    #Memorize some \"precision\" information only if E outperfom P\n",
    "    if(outperforms_prec):\n",
    "        outperf_pairs_prec.append(pairs[i])\n",
    "        temp = pairs[i][1] + pairs[i][0]\n",
    "        pairs_set_prec.append(''.join(str(x) for x in temp))\n",
    "        d_measure_prec.append(delta_prec)\n",
    "    \n",
    "    #Memorize some \"DCG\" information only if E outperfom P\n",
    "    if(outperforms_DCG):\n",
    "        outperf_pairs_DCG.append(pairs[i])\n",
    "        temp = pairs[i][1] + pairs[i][0]\n",
    "        pairs_set_DCG.append(''.join(str(x) for x in temp))\n",
    "        d_measure_DCG.append(delta_DCG)\n",
    "    \n",
    "    #Memorize some \"RBP\" information only if E outperfom P\n",
    "    if(outperforms_RBP):\n",
    "        outperf_pairs_RBP.append(pairs[i])\n",
    "        temp = pairs[i][1] + pairs[i][0]\n",
    "        pairs_set_RBP.append(''.join(str(x) for x in temp))\n",
    "        d_measure_RBP.append(delta_RBP)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs for which E outperforms P for all three measures:  17184\n",
      "Pair in precision and in DCG: ['1102100122']\n",
      "Pair in precision and not in DCG: ['1111222102']\n",
      "Pair in DCG and not in precision: ['2222212111']\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "\n",
    "print (\"Number of pairs for which E outperforms P for all three measures: \",len(set.intersection(set(pairs_set_prec), set(pairs_set_DCG), set(pairs_set_RBP))))\n",
    "print (\"Pair in precision and in DCG:\",rand.sample(set.intersection(set(pairs_set_prec), set(pairs_set_DCG)),1))\n",
    "\n",
    "# pick result that is in prec but is not in DCG\n",
    "print (\"Pair in precision and not in DCG:\",rand.sample(set(pairs_set_prec) - set(pairs_set_DCG),1))\n",
    "# pick result that is in DCG but is not in prec\n",
    "print (\"Pair in DCG and not in precision:\",rand.sample(set(pairs_set_DCG) - set(pairs_set_prec),1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pair in precision and not in DCG:', [[1, 1, 1, 1, 2], [2, 2, 1, 0, 2]], 'precision E:', 1.0, 'precision P:', 0.75, 'DCG E: ', 2.5616063116448506, 'DCG P: ', 5.3927892607143724)\n",
      "('Pair in DCG and not in precision:', [[2, 2, 2, 2, 2], [1, 2, 1, 1, 1]], 'DCG E: ', 7.6848189349345519, 'DCG P: ', 3.8234658187877653, 'precision E:', 1.0, 'precision P:', 1.0)\n"
     ]
    }
   ],
   "source": [
    "pair_prec_not_DCG = [[1,1,1,1,2],[2,2,1,0,2]] # pair E, P \n",
    "pair_DCG_not_prec = [[2,2,2,2,2],[1,2,1,1,1]] # pair E, P \n",
    "print(\"Pair in precision and not in DCG:\",pair_prec_not_DCG, \"precision E:\",\n",
    "      Binary_measure_precision(pair_prec_not_DCG[0],rank),\"precision P:\",\n",
    "      Binary_measure_precision(pair_prec_not_DCG[1],rank),\"DCG E: \",\n",
    "     DCG(pair_prec_not_DCG[0],rank),\"DCG P: \",\n",
    "     DCG(pair_prec_not_DCG[1],rank) )\n",
    "\n",
    "print(\"Pair in DCG and not in precision:\",pair_DCG_not_prec, \"DCG E: \",\n",
    "     DCG(pair_DCG_not_prec[0],rank),\"DCG P: \",\n",
    "     DCG(pair_DCG_not_prec[1],rank),\"precision E:\",\n",
    "      Binary_measure_precision(pair_DCG_not_prec[0],rank),\"precision P:\",\n",
    "      Binary_measure_precision(pair_DCG_not_prec[1],rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanations\n",
    "For precision, DCG, and RBP, we calculate the difference between all combination of E and P that we have generated in Step 1, then we append every difference that is greater than zero, which means that E outperforms P.\n",
    "##### Analysis\n",
    "For each three measures, we have different pairs in which E outperforms P. There are 17184 pairs where E outperforms P in all three measures. However, there exists some pairs where E outperforms P for one kind of measure, but not the other. <br> \n",
    "For example, E=[1,1,1,1,2] outperforms P=[2,2,1,0,2] in precision (delta of 0.25), but not in DCG (delta of -2.83118294971) . This is due to the fact that precision does not check order and the degree of relevance. The same happens the opposite way : E=[2,2,2,2,2] outperforms P=[1,2,1,1,1] in DCG (delta of 3.86135311614), but not in precision (delta of 0). <br>\n",
    "This indicates that in real world application, it is important to implement different kinds of measure so we will have broader feedback to optimize the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 4 : Implement Interleave (15 points)\n",
    "\n",
    "Implement 2 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, AND (2),\n",
    "Probabilistic Interleaving. The interleaving algorithms should (a) given two rankings of relevance\n",
    "interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign\n",
    "credit to the algorithms that produced the rankings.\n",
    "\n",
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assignment assumption : E and P always return different documents\n",
    "# general parameter : consider two pairs can contain same documents \n",
    "\n",
    "def team_draft_interleave(ranking_pairs, general=False, clicks=[]):\n",
    "    # interleave two list\n",
    "    interleaved_list = []\n",
    "    added_list = []\n",
    "    assert len(ranking_pairs[0]) ==  len(ranking_pairs[1])\n",
    "    for i, elem in enumerate(ranking_pairs[0]):\n",
    "        coin = rand.choice([0,1])\n",
    "        if(coin == 0):\n",
    "            if(general and ranking_pairs[0][i] not in added_list):\n",
    "                interleaved_list.append((ranking_pairs[0][i], \"E\"))\n",
    "                added_list.append(ranking_pairs[0][i])\n",
    "            if(general and ranking_pairs[1][i] not in added_list):\n",
    "                interleaved_list.append((ranking_pairs[1][i], \"P\"))\n",
    "                added_list.append(ranking_pairs[1][i])\n",
    "            if(not general):\n",
    "                interleaved_list.append((ranking_pairs[0][i], \"E\"))\n",
    "                interleaved_list.append((ranking_pairs[1][i], \"P\"))\n",
    "        if(coin == 1):\n",
    "            if(general and ranking_pairs[1][i] not in added_list):\n",
    "                interleaved_list.append((ranking_pairs[1][i], \"P\"))\n",
    "                added_list.append(ranking_pairs[1][i])\n",
    "            if(general and ranking_pairs[0][i] not in added_list):\n",
    "                interleaved_list.append((ranking_pairs[0][i], \"E\"))\n",
    "                added_list.append(ranking_pairs[0][i])\n",
    "            if(not general):\n",
    "                interleaved_list.append((ranking_pairs[1][i], \"P\"))\n",
    "                interleaved_list.append((ranking_pairs[0][i], \"E\"))\n",
    "    # generate user clicks randomly\n",
    "    if(not general):\n",
    "        user_click = [rand.randrange(0, 2) for _ in range(0, len(interleaved_list))]\n",
    "    else:\n",
    "        user_click = clicks\n",
    "    # calculate scores\n",
    "    E_wins, P_wins = 0, 0\n",
    "    for i, click in enumerate(user_click):\n",
    "        if click == 1:\n",
    "            if interleaved_list[i][1] == \"E\":\n",
    "                E_wins += 1\n",
    "            if interleaved_list[i][1] == \"P\":\n",
    "                P_wins += 1\n",
    "    return interleaved_list, user_click, E_wins, P_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Interleaved list ', [(2, 'P'), (0, 'E'), (1, 'E'), (1, 'P'), (0, 'E'), (2, 'P'), (0, 'E'), (2, 'P'), (2, 'E'), (2, 'P')], 'user click', [0, 0, 1, 0, 1, 1, 1, 1, 1, 1], 'number of E wins', 4, 'number of P wins', 3)\n"
     ]
    }
   ],
   "source": [
    "r = rand.randint(0,59049-1)\n",
    "interleaved_list, user_click, E_wins, P_wins = team_draft_interleave(pairs[r])\n",
    "print(\"Interleaved list \",interleaved_list,\"user click\" ,user_click,\"number of E wins\" ,E_wins,\"number of P wins\" ,P_wins )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(6, 'P'),\n",
       "  (1, 'E'),\n",
       "  (2, 'E'),\n",
       "  (7, 'P'),\n",
       "  (8, 'P'),\n",
       "  (3, 'E'),\n",
       "  (4, 'E'),\n",
       "  (5, 'E')],\n",
       " [1, 0, 0, 1, 0],\n",
       " 0,\n",
       " 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# general test\n",
    "general_test = ([1,2,3,4,5], [6,7,8,1,2])\n",
    "team_draft_interleave(general_test, True, [1,0,0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 1, 2, 4, 10]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# tested the result with existing library (https://github.com/mpkato/interleaving/tree/master/interleaving)\n",
    "\n",
    "import interleaving\n",
    "a = [1, 2, 3, 4, 5] # Ranking 1\n",
    "b = [6, 7, 8, 9, 10] # Ranking 2\n",
    "method = interleaving.Probabilistic([a, b]) # initialize an interleaving method\n",
    "ranking = method.interleave() # interleaving\n",
    "print ranking\n",
    "\n",
    "clicks = [0, 3] # observed clicks, i.e. documents 1 and 2 are clicked\n",
    "result = interleaving.Probabilistic.evaluate(ranking, clicks)\n",
    "print result # (0, 1) indicates Ranking 1 won Ranking 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def softmax(tau, length):\n",
    "    total_denominator = 0\n",
    "    softmax_distribution = []\n",
    "    for i in range(1, length+1):\n",
    "        total_denominator += float(1.0/np.power(i,tau))\n",
    "    for i in range(1, length+1):\n",
    "        softmax_distribution.append(float(1.0/np.power(i,tau) / (total_denominator)))\n",
    "    return softmax_distribution\n",
    "\n",
    "def probabilistic_interleave(ranking_pairs):\n",
    "    # generate softmax probability distribution with tau = 3 (Hofmann, 2011)\n",
    "    assert len(ranking_pairs[0]) ==  len(ranking_pairs[1])\n",
    "    E = copy.copy(ranking_pairs[0])\n",
    "    P = copy.copy(ranking_pairs[1])\n",
    "    softmax_distribution_E = softmax(3,len(E))\n",
    "    softmax_distribution_P = softmax(3,len(P))\n",
    "    # interleave two list with softmax probability distribution\n",
    "    interleaved_list = []\n",
    "    for _ in range(len(E) + len(P)):\n",
    "        coin = rand.choice([0,1])\n",
    "        if(coin == 0):\n",
    "            # check if E is empty then just sample from P\n",
    "            if not E:\n",
    "                # choose element from P randomly\n",
    "                picked = np.random.choice(P, 1, p=softmax_distribution_P)[0]\n",
    "                # append it to interleaved list\n",
    "                interleaved_list.append((picked, \"P\"))\n",
    "                # remove the picked element\n",
    "                P.remove(picked)\n",
    "                # renormalize the distribution\n",
    "                softmax_distribution_P = softmax(3,len(P))\n",
    "            # else sample from E\n",
    "            else:\n",
    "                # choose element from E randomly\n",
    "                picked = np.random.choice(E, 1, p=softmax_distribution_E)[0]\n",
    "                # append it to interleaved list\n",
    "                interleaved_list.append((picked, \"E\"))\n",
    "                # remove the picked element\n",
    "                E.remove(picked)\n",
    "                # renormalize the distribution\n",
    "                softmax_distribution_E = softmax(3,len(E))\n",
    "        if(coin == 1):\n",
    "            # check if P is empty then just sample from E\n",
    "            if not P:\n",
    "                # choose element from E randomly\n",
    "                picked = np.random.choice(E, 1, p=softmax_distribution_E)[0]\n",
    "                # append it to interleaved list\n",
    "                interleaved_list.append((picked, \"E\"))\n",
    "                # remove the picked element\n",
    "                E.remove(picked)\n",
    "                # renormalize the distribution\n",
    "                softmax_distribution_E = softmax(3,len(E))\n",
    "            # else sample from P\n",
    "            else:\n",
    "                # choose element from P randomly\n",
    "                picked = np.random.choice(P, 1, p=softmax_distribution_P)[0]\n",
    "                # append it to interleaved list\n",
    "                interleaved_list.append((picked, \"P\"))\n",
    "                # remove the picked element\n",
    "                P.remove(picked)\n",
    "                # renormalize the distribution\n",
    "                softmax_distribution_P = softmax(3,len(P))\n",
    "    # generate user clicks randomly\n",
    "    user_click = [rand.randrange(0, 2) for _ in range(0, len(interleaved_list))]\n",
    "    # calculate scores\n",
    "    E_wins, P_wins = 0, 0\n",
    "    for i, click in enumerate(user_click):\n",
    "        if click == 1:\n",
    "            if interleaved_list[i][1] == \"E\":\n",
    "                E_wins += 1\n",
    "            if interleaved_list[i][1] == \"P\":\n",
    "                P_wins += 1\n",
    "    return interleaved_list, user_click, E_wins, P_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = rand.randint(0,59049-1)\n",
    "#team_draft_interleave(pairs[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(2, 'P'),\n",
       "  (2, 'E'),\n",
       "  (2, 'P'),\n",
       "  (1, 'P'),\n",
       "  (1, 'E'),\n",
       "  (0, 'P'),\n",
       "  (0, 'P'),\n",
       "  (1, 'E'),\n",
       "  (1, 'E'),\n",
       "  (1, 'E')],\n",
       " [0, 0, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       " 2,\n",
       " 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilistic_interleave(pairs[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do probabilistic interleave : \n",
    "- Make it general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Explanations\n",
    "\n",
    "##### Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Implement User Clicks Simulation ( 25 points)\n",
    "\n",
    "Having interleaved all the ranking pairs an online experiment could be ran. However, given that we do not\n",
    "have any users (and the entire homework is a big simulation) we will simulate user clicks.<br>\n",
    "We have considered a number of click models including:<br>\n",
    "* 1. Random Click Model (RCM)\n",
    "* 2. Position-Based Model (PBM)\n",
    "* 3. Simple Dependent Click Model (SDCM)\n",
    "* 4. Simple Dynamic Bayesian Network (SDBN)\n",
    "\n",
    "Consider two different click models, (a) the Random Click Model (RCM), and (b) one out of the\n",
    "remaining 3 aforementioned models. The parameters of some of these models can be estimated using the\n",
    "Maximum Likelihood Estimation (MLE) method, while others require using the\n",
    "Expectation-Maximization (EM) method. Implement the two models so that:\n",
    "* (a) there is a method that learns the parameters of the model given a set of training data, \n",
    "* (b) there is a method that predicts the click probability given a ranked list of relevance labels, \n",
    "* (c) there is a method that decides - stochastically -whether a document is clicked based on these probabilities.<br>\n",
    "\n",
    "Having implemented the three click models, estimate the model parameters using the Yandex Click Log\n",
    "\n",
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Random Click Model (RCM) \n",
    "def learns_param_RCM(training_data):\n",
    "    numb_doc = 0\n",
    "    num_click=0\n",
    "    for data in training_data:\n",
    "        numb_doc+=1\n",
    "        if(\"C\" in data[2]):\n",
    "            num_click+=1\n",
    "    return float(num_click)/numb_doc  \n",
    "\n",
    "def predict_click_proba_RCM(training_data,relevance_labels) :\n",
    "    rau = learns_param_RCM(training_data)\n",
    "    #since the probability is rau for every document, we return a list of rau that has the length of the relevance labels list\n",
    "    return [rau]*len(relevance_labels)\n",
    "\n",
    "def is_doc_clicked_RCM(training_data,relevance_labels,document):\n",
    "    proba_distrib= predict_click_proba_RCM(training_data,relevance_labels)\n",
    "    return (bernoulli.rvs(proba_distrib[document])==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isDocClicked False\n"
     ]
    }
   ],
   "source": [
    "#open file and preprocess data\n",
    "training_data=[]\n",
    "relevance_labels= [1,0,2,5,0,1,1,2,5]\n",
    "document= 3\n",
    "\n",
    "f= open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "for line in f:\n",
    "    training_data.append(line.split(\"\\n\")[0].split('\\t'))\n",
    "f.close()\n",
    "\n",
    "print(\"isDocClicked\", is_doc_clicked_RCM(training_data,relevance_labels,document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanations\n",
    "\n",
    "##### Analysis"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
